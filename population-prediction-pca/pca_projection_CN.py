#!/usr/bin/env python3
"""
PCAæŠ•å½±è„šæœ¬ - å°†æ–°æ ·æœ¬æŠ•å½±åˆ°å·²è®­ç»ƒçš„PCAç©ºé—´

ç”¨äºç¥–æºæ¨æ–­ï¼šå°†æœªçŸ¥ç§æ—çš„æ ·æœ¬æŠ•å½±åˆ°1000 Genomes PCAç©ºé—´ï¼Œ
å¹¶æ¨æ–­å…¶ç§æ—å½’å±ã€‚
"""

import numpy as np
import pandas as pd
import allel
import pickle
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import os
from pathlib import Path
from tqdm import tqdm
from multiprocessing import Pool
import traceback

# å°è¯•å¯¼å…¥plotly
try:
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False


def load_model_and_snps(model_file, snps_file):
    """
    åŠ è½½PCAæ¨¡å‹å’ŒSNPä½ç½®åˆ—è¡¨
    """
    print("=" * 80)
    print("åŠ è½½PCAæ¨¡å‹å’ŒSNPä½ç½®...")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“‚ åŠ è½½PCAæ¨¡å‹: {model_file}")
    with open(model_file, 'rb') as f:
        model_data = pickle.load(f)
    
    pca_model = model_data['pca_model']
    mean_vals = model_data['mean']
    std_vals = model_data['std']
    n_components = model_data['n_components']
    n_features = model_data['n_features']
    
    print(f"  âœ“ ä¸»æˆåˆ†æ•°: {n_components}")
    print(f"  âœ“ ç‰¹å¾æ•°: {n_features}")
    print(f"  âœ“ æ–¹å·®è§£é‡Šç‡: {sum(pca_model.explained_variance_ratio_)*100:.2f}%")
    
    # åŠ è½½SNPä½ç½®
    print(f"\nğŸ“‚ åŠ è½½SNPä½ç½®åˆ—è¡¨: {snps_file}")
    snp_data = np.load(snps_file)
    positions = snp_data['positions']
    
    print(f"  âœ“ SNPæ•°é‡: {len(positions)}")
    
    # æ£€æŸ¥positionså’Œn_featuresæ˜¯å¦ä¸€è‡´
    if len(positions) != n_features:
        print(f"\nâŒ é”™è¯¯: SNPä½ç½®æ•°({len(positions)})ä¸æ¨¡å‹ç‰¹å¾æ•°({n_features})ä¸ä¸€è‡´")
        print(f"\nå¯èƒ½åŸå› :")
        print(f"  1. ä½¿ç”¨çš„æ˜¯æ—§ç‰ˆæœ¬è®­ç»ƒè„šæœ¬ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶")
        print(f"  2. snps.npz å’Œ model.pkl æ¥è‡ªä¸åŒçš„è®­ç»ƒè¿è¡Œ")
        print(f"\nè§£å†³æ–¹æ¡ˆ:")
        print(f"  ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„è®­ç»ƒè„šæœ¬é‡æ–°è®­ç»ƒï¼ˆå·²ä¿®å¤æ­¤bugï¼‰:")
        print(f"  python3 1000genomes_pca_ultimate.py ... \\")
        print(f"      --save-snps snps.npz --save-model model.pkl")
        
        raise ValueError(f"SNPä½ç½®åˆ—è¡¨ä¸æ¨¡å‹ä¸åŒ¹é…ï¼Œè¯·é‡æ–°è®­ç»ƒ")
    
    return pca_model, mean_vals, std_vals, positions


def normalize_chrom_name(chrom):
    """æ ‡å‡†åŒ–æŸ“è‰²ä½“åç§°ï¼Œå»é™¤chrå‰ç¼€"""
    chrom_str = str(chrom)
    if chrom_str.startswith('chr'):
        return chrom_str[3:]
    return chrom_str


def decode_position_with_chrom(position_offset):
    """
    ä»offsetç¼–ç çš„ä½ç½®è§£ç å‡ºæŸ“è‰²ä½“å’Œå®é™…ä½ç½®
    
    è®­ç»ƒæ—¶ç¼–ç è§„åˆ™ï¼š
    position_offset = chromosome * 1_000_000_000 + actual_position
    """
    chrom = int(position_offset // 1_000_000_000)
    actual_pos = int(position_offset % 1_000_000_000)
    return chrom, actual_pos


def extract_genotypes_from_vcf(vcf_file, positions, region=None):
    """
    ä»VCFæ–‡ä»¶æå–æŒ‡å®šä½ç½®çš„åŸºå› å‹
    æ”¯æŒè‡ªåŠ¨å¤„ç†æŸ“è‰²ä½“å‘½åå·®å¼‚ï¼ˆchr1 vs 1ï¼‰
    
    å‚æ•°:
        vcf_file: VCFæ–‡ä»¶è·¯å¾„
        positions: éœ€è¦æå–çš„ä½ç½®åˆ—è¡¨ï¼ˆå·²ç¼–ç offsetï¼‰
        region: æŸ“è‰²ä½“åŒºåŸŸï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        genotypes: åŸºå› å‹çŸ©é˜µ (samples x variants)
        samples: æ ·æœ¬IDåˆ—è¡¨
        matched_positions: å®é™…åŒ¹é…åˆ°çš„ä½ç½®
    """
    print("\n" + "=" * 80)
    print("ä»VCFæå–åŸºå› å‹æ•°æ®...")
    print("=" * 80)
    
    print(f"\nğŸ“‚ è¯»å–VCF: {vcf_file}")
    if region:
        print(f"  åŒºåŸŸ: {region}")
    
    # è¯»å–VCF
    try:
        callset = allel.read_vcf(vcf_file, region=region)
    except Exception as e:
        raise ValueError(f"è¯»å–VCFå¤±è´¥: {e}")
    
    if callset is None or 'calldata/GT' not in callset:
        raise ValueError("VCFæ–‡ä»¶æ— æ•°æ®æˆ–ç¼ºå°‘åŸºå› å‹ä¿¡æ¯")
    
    # è·å–VCFä¸­çš„æŸ“è‰²ä½“å’Œä½ç½®
    vcf_chroms = callset['variants/CHROM']
    vcf_positions = callset['variants/POS'].astype(np.int64)
    genotypes = allel.GenotypeArray(callset['calldata/GT'])
    samples = callset.get('samples', None)
    
    if samples is None:
        raise ValueError("VCFæ–‡ä»¶ç¼ºå°‘æ ·æœ¬ä¿¡æ¯")
    
    # ç¡®ä¿samplesæ˜¯å­—ç¬¦ä¸²
    if isinstance(samples[0], bytes):
        samples = np.array([s.decode('utf-8') for s in samples])
    else:
        samples = np.array([str(s) for s in samples])
    
    print(f"  VCFä¸­çš„æ ·æœ¬æ•°: {len(samples)}")
    print(f"  VCFä¸­çš„å˜å¼‚æ•°: {len(vcf_positions)}")
    
    # æ£€æµ‹VCFæŸ“è‰²ä½“å‘½åæ ¼å¼
    first_chrom = vcf_chroms[0].decode('utf-8') if isinstance(vcf_chroms[0], bytes) else str(vcf_chroms[0])
    vcf_has_chr_prefix = first_chrom.startswith('chr')
    
    print(f"  æŸ“è‰²ä½“å‘½åæ ¼å¼: {first_chrom} ({'æœ‰chrå‰ç¼€' if vcf_has_chr_prefix else 'æ— chrå‰ç¼€'})")
    
    # æ£€æµ‹å‚è€ƒä½ç½®çš„æ ¼å¼ï¼ˆä»ç¬¬ä¸€ä¸ªä½ç‚¹è§£ç ï¼‰
    ref_chrom_example, ref_pos_example = decode_position_with_chrom(positions[0])
    print(f"  å‚è€ƒæ•°æ®æ ¼å¼: æŸ“è‰²ä½“{ref_chrom_example} (æ— chrå‰ç¼€)")
    
    # å†³å®šæ˜¯å¦éœ€è¦è½¬æ¢
    need_normalize = vcf_has_chr_prefix
    
    if need_normalize:
        print(f"  ğŸ”„ è‡ªåŠ¨è½¬æ¢: å°†VCFçš„'chr{ref_chrom_example}'æ ¼å¼è½¬ä¸º'{ref_chrom_example}'æ ¼å¼")
    else:
        print(f"  âœ“ å‘½åæ ¼å¼ä¸€è‡´ï¼Œæ— éœ€è½¬æ¢")
    
    # æ ‡å‡†åŒ–VCFçš„æŸ“è‰²ä½“åç§°ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if need_normalize:
        if isinstance(vcf_chroms[0], bytes):
            vcf_chroms_normalized = np.array([normalize_chrom_name(c.decode('utf-8')) for c in vcf_chroms])
        else:
            vcf_chroms_normalized = np.array([normalize_chrom_name(c) for c in vcf_chroms])
    else:
        if isinstance(vcf_chroms[0], bytes):
            vcf_chroms_normalized = np.array([c.decode('utf-8') for c in vcf_chroms])
        else:
            vcf_chroms_normalized = np.array([str(c) for c in vcf_chroms])
    
    # åˆ›å»ºVCFçš„æŸ“è‰²ä½“+ä½ç½®ç´¢å¼•
    vcf_chrom_pos_dict = {}
    for i, (chrom, pos) in enumerate(zip(vcf_chroms_normalized, vcf_positions)):
        key = (chrom, pos)
        vcf_chrom_pos_dict[key] = i
    
    # åŒ¹é…ä½ç½®
    print(f"\nğŸ” åŒ¹é…SNPä½ç½®...")
    print(f"  å‚è€ƒä½ç‚¹æ•°: {len(positions)}")
    
    matched_indices = []
    matched_positions = []
    
    print(f"  æ­£åœ¨åŒ¹é…ä½ç‚¹ï¼ˆè‡ªåŠ¨å¤„ç†æŸ“è‰²ä½“å‘½åå·®å¼‚ï¼‰...")
    for i, pos_offset in tqdm(enumerate(positions), total=len(positions), desc="  åŒ¹é…è¿›åº¦"):
        # ä»offsetè§£ç å‡ºæŸ“è‰²ä½“å’Œå®é™…ä½ç½®
        chrom, actual_pos = decode_position_with_chrom(pos_offset)
        chrom_str = str(chrom)
        
        # åœ¨VCFä¸­æŸ¥æ‰¾
        key = (chrom_str, actual_pos)
        if key in vcf_chrom_pos_dict:
            vcf_idx = vcf_chrom_pos_dict[key]
            matched_indices.append((i, vcf_idx))
            matched_positions.append(pos_offset)
    
    n_matched = len(matched_indices)
    match_rate = n_matched / len(positions)
    
    print(f"  åŒ¹é…ä½ç‚¹æ•°: {n_matched}")
    print(f"  åŒ¹é…ç‡: {match_rate*100:.1f}%")
    
    if match_rate < 0.5:
        print(f"\nâš ï¸  è­¦å‘Š: åŒ¹é…ç‡è¿‡ä½ ({match_rate*100:.1f}%)")
        print(f"  å»ºè®®: æ£€æŸ¥VCFæ–‡ä»¶æ˜¯å¦ä½¿ç”¨ç›¸åŒçš„å‚è€ƒåŸºå› ç»„")
    elif match_rate < 0.8:
        print(f"\nâš ï¸  æ³¨æ„: åŒ¹é…ç‡è¾ƒä½ ({match_rate*100:.1f}%)ï¼Œç»“æœå¯èƒ½ä¸å¤Ÿå‡†ç¡®")
    
    if n_matched == 0:
        raise ValueError("æ²¡æœ‰åŒ¹é…çš„SNPä½ç‚¹ï¼è¯·æ£€æŸ¥VCFæ–‡ä»¶å’Œå‚è€ƒä½ç‚¹åˆ—è¡¨")
    
    # æå–åŒ¹é…ä½ç‚¹çš„åŸºå› å‹
    print(f"\nğŸ“Š æå–åŸºå› å‹æ•°æ®...")
    ref_indices = [idx[0] for idx in matched_indices]
    vcf_indices = [idx[1] for idx in matched_indices]
    
    # åˆ›å»ºå®Œæ•´çš„åŸºå› å‹çŸ©é˜µï¼ˆåŒ…å«ç¼ºå¤±ä½ç‚¹ï¼‰
    n_samples = len(samples)
    n_ref_snps = len(positions)
    full_genotypes = np.full((n_samples, n_ref_snps), -1, dtype=np.int8)
    
    # å¡«å……åŒ¹é…çš„ä½ç‚¹
    matched_genotypes = genotypes[vcf_indices].to_n_alt().T
    for i, ref_idx in enumerate(ref_indices):
        full_genotypes[:, ref_idx] = matched_genotypes[:, i]
    
    print(f"  æå–å®Œæˆ: {n_samples} æ ·æœ¬ Ã— {n_ref_snps} ä½ç‚¹")
    print(f"  ç¼ºå¤±ä½ç‚¹æ•°: {n_ref_snps - n_matched}")
    
    return full_genotypes, samples, np.array(matched_positions)


def project_to_pca(genotypes, pca_model, mean_vals, std_vals):
    """
    å°†æ–°æ ·æœ¬æŠ•å½±åˆ°PCAç©ºé—´
    
    å‚æ•°:
        genotypes: åŸºå› å‹çŸ©é˜µ (samples x variants)
        pca_model: è®­ç»ƒå¥½çš„PCAæ¨¡å‹
        mean_vals: æ ‡å‡†åŒ–å‡å€¼
        std_vals: æ ‡å‡†åŒ–æ ‡å‡†å·®
    
    è¿”å›:
        pca_coords: PCAåæ ‡ (samples x n_components)
    """
    print("\n" + "=" * 80)
    print("æŠ•å½±åˆ°PCAç©ºé—´...")
    print("=" * 80)
    
    n_samples, n_variants = genotypes.shape
    print(f"\nè¾“å…¥: {n_samples} æ ·æœ¬ Ã— {n_variants} å˜å¼‚")
    
    # å¤„ç†ç¼ºå¤±å€¼ï¼ˆç”¨å‡å€¼å¡«å……ï¼‰
    print(f"\nå¤„ç†ç¼ºå¤±å€¼...")
    genotypes_clean = genotypes.copy().astype(float)
    
    missing_count = np.sum(genotypes_clean == -1)
    missing_rate = missing_count / (n_samples * n_variants)
    print(f"  ç¼ºå¤±å€¼æ•°é‡: {missing_count}")
    print(f"  ç¼ºå¤±ç‡: {missing_rate*100:.2f}%")
    
    print(f"  å¡«å……ç¼ºå¤±å€¼...")
    for i in tqdm(range(n_variants), desc="  å¡«å……è¿›åº¦"):
        col = genotypes_clean[:, i]
        mask = col != -1
        if np.sum(mask) > 0:
            mean_val = np.mean(col[mask])
            col[~mask] = mean_val
        else:
            # å¦‚æœæ•´åˆ—éƒ½ç¼ºå¤±ï¼Œç”¨å‚è€ƒå‡å€¼å¡«å……
            col[:] = mean_vals[i]
    
    # åº”ç”¨ä¸å‚è€ƒäººç¾¤ç›¸åŒçš„æ ‡å‡†åŒ–
    print(f"\nåº”ç”¨æ ‡å‡†åŒ–...")
    genotypes_scaled = (genotypes_clean - mean_vals) / std_vals
    
    # æŠ•å½±
    print(f"\næ‰§è¡ŒPCAæŠ•å½±...")
    pca_coords = pca_model.transform(genotypes_scaled)
    
    print(f"  âœ“ æŠ•å½±å®Œæˆ")
    print(f"  è¾“å‡º: {pca_coords.shape[0]} æ ·æœ¬ Ã— {pca_coords.shape[1]} ä¸»æˆåˆ†")
    
    return pca_coords


def infer_ancestry(query_pca, reference_pca, reference_populations, n_neighbors=20):
    """
    ä½¿ç”¨KNNæ¨æ–­ç§æ—å½’å±
    
    å‚æ•°:
        query_pca: æŸ¥è¯¢æ ·æœ¬çš„PCAåæ ‡
        reference_pca: å‚è€ƒäººç¾¤çš„PCAåæ ‡
        reference_populations: å‚è€ƒäººç¾¤çš„ç§æ—æ ‡ç­¾
        n_neighbors: KNNçš„é‚»å±…æ•°
    
    è¿”å›:
        predictions: é¢„æµ‹çš„ç§æ—
        probabilities: å„ç§æ—çš„æ¦‚ç‡
    """
    print("\n" + "=" * 80)
    print("æ¨æ–­ç§æ—å½’å±...")
    print("=" * 80)
    
    print(f"\nä½¿ç”¨KNNåˆ†ç±»å™¨ (K={n_neighbors})")
    print(f"  å‚è€ƒæ ·æœ¬æ•°: {len(reference_pca)}")
    print(f"  æŸ¥è¯¢æ ·æœ¬æ•°: {len(query_pca)}")
    
    # è®­ç»ƒKNN
    knn = KNeighborsClassifier(n_neighbors=n_neighbors)
    knn.fit(reference_pca, reference_populations)
    
    # é¢„æµ‹
    predictions = knn.predict(query_pca)
    probabilities = knn.predict_proba(query_pca)
    
    # ç»Ÿè®¡
    unique_pops, counts = np.unique(predictions, return_counts=True)
    print(f"\né¢„æµ‹ç»“æœç»Ÿè®¡:")
    for pop, count in zip(unique_pops, counts):
        print(f"  {pop}: {count} æ ·æœ¬ ({count/len(predictions)*100:.1f}%)")
    
    return predictions, probabilities, knn.classes_


def generate_ancestry_report(samples, predictions, probabilities, pop_labels, output_file):
    """
    ç”Ÿæˆè¯¦ç»†çš„ç¥–æºæ¨æ–­æŠ¥å‘Š
    """
    print(f"\nç”Ÿæˆç¥–æºæ¨æ–­æŠ¥å‘Š: {output_file}")
    
    with open(output_file, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("ç¥–æºæ¨æ–­æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"æ ·æœ¬æ•°: {len(samples)}\n")
        f.write(f"å‚è€ƒäººç¾¤: {', '.join(pop_labels)}\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("è¯¦ç»†ç»“æœ\n")
        f.write("=" * 80 + "\n\n")
        
        for i, sample in enumerate(samples):
            pred_pop = predictions[i]
            probs = probabilities[i]
            confidence = np.max(probs)
            
            f.write(f"æ ·æœ¬: {sample}\n")
            f.write(f"  é¢„æµ‹ç§æ—: {pred_pop}\n")
            f.write(f"  ç½®ä¿¡åº¦: {confidence*100:.1f}%\n")
            f.write(f"  æ¦‚ç‡åˆ†å¸ƒ:\n")
            
            for pop, prob in zip(pop_labels, probs):
                f.write(f"    {pop}: {prob*100:.1f}%\n")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ··åˆäººç¾¤
            sorted_probs = sorted(zip(pop_labels, probs), key=lambda x: x[1], reverse=True)
            if sorted_probs[1][1] > 0.2:  # ç¬¬äºŒé«˜çš„æ¦‚ç‡>20%
                f.write(f"  æ³¨é‡Š: å¯èƒ½ä¸ºæ··åˆäººç¾¤ ({sorted_probs[0][0]}/{sorted_probs[1][0]})\n")
            
            f.write("\n")
    
    print(f"  âœ“ æŠ¥å‘Šå·²ä¿å­˜")


def plot_combined_pca(reference_pca, reference_populations, query_pca, 
                      query_samples, predictions, probabilities,
                      output_prefix='projection'):
    """
    ç»˜åˆ¶å‚è€ƒäººç¾¤å’ŒæŸ¥è¯¢æ ·æœ¬çš„ç»„åˆPCAå›¾
    """
    print("\n" + "=" * 80)
    print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("=" * 80)
    
    # é¢œè‰²å®šä¹‰
    pop_colors = {
        'AFR': '#E74C3C',  # çº¢è‰²
        'AMR': '#9B59B6',  # ç´«è‰²
        'EAS': '#3498DB',  # è“è‰²
        'EUR': '#F39C12',  # æ©™è‰²
        'SAS': '#2ECC71'   # ç»¿è‰²
    }
    
    # 2Då›¾
    print(f"\nç”Ÿæˆ2Dæ•£ç‚¹å›¾...")
    fig, ax = plt.subplots(figsize=(16, 12))
    
    # ç»˜åˆ¶å‚è€ƒäººç¾¤ï¼ˆå°åœ†ç‚¹ï¼‰
    for pop in np.unique(reference_populations):
        mask = reference_populations == pop
        ax.scatter(
            reference_pca[mask, 0],
            reference_pca[mask, 1],
            c=pop_colors.get(pop, '#999999'),
            label=f'{pop} (å‚è€ƒ)',
            alpha=0.4,
            s=30,
            marker='o'
        )
    
    # ç»˜åˆ¶æŸ¥è¯¢æ ·æœ¬ï¼ˆæ˜Ÿå·ï¼‰
    for pop in np.unique(predictions):
        mask = predictions == pop
        ax.scatter(
            query_pca[mask, 0],
            query_pca[mask, 1],
            c=pop_colors.get(pop, '#999999'),
            label=f'{pop} (æŸ¥è¯¢)',
            alpha=0.9,
            s=150,
            marker='*',
            edgecolors='black',
            linewidths=1.5
        )
    
    ax.set_xlabel('PC1', fontsize=14, fontweight='bold')
    ax.set_ylabel('PC2', fontsize=14, fontweight='bold')
    ax.set_title('PCAæŠ•å½± - å‚è€ƒäººç¾¤ä¸æŸ¥è¯¢æ ·æœ¬', fontsize=16, fontweight='bold')
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_file = f'{output_prefix}_2d.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"  ä¿å­˜: {output_file}")
    plt.close()
    
    # 3Då›¾
    if reference_pca.shape[1] >= 3:
        print(f"\nç”Ÿæˆ3Dæ•£ç‚¹å›¾...")
        fig = plt.figure(figsize=(18, 14))
        ax = fig.add_subplot(111, projection='3d')
        
        # å‚è€ƒäººç¾¤
        for pop in np.unique(reference_populations):
            mask = reference_populations == pop
            ax.scatter(
                reference_pca[mask, 0],
                reference_pca[mask, 1],
                reference_pca[mask, 2],
                c=pop_colors.get(pop, '#999999'),
                label=f'{pop} (å‚è€ƒ)',
                alpha=0.3,
                s=30,
                marker='o'
            )
        
        # æŸ¥è¯¢æ ·æœ¬
        for pop in np.unique(predictions):
            mask = predictions == pop
            ax.scatter(
                query_pca[mask, 0],
                query_pca[mask, 1],
                query_pca[mask, 2],
                c=pop_colors.get(pop, '#999999'),
                label=f'{pop} (æŸ¥è¯¢)',
                alpha=0.9,
                s=150,
                marker='*',
                edgecolors='black',
                linewidths=1.5
            )
        
        ax.set_xlabel('PC1', fontweight='bold')
        ax.set_ylabel('PC2', fontweight='bold')
        ax.set_zlabel('PC3', fontweight='bold')
        ax.set_title('PCAæŠ•å½± - 3Dè§†å›¾', fontsize=16, fontweight='bold')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        
        plt.tight_layout()
        output_file = f'{output_prefix}_3d.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"  ä¿å­˜: {output_file}")
        plt.close()
        
        # äº¤äº’å¼3Då›¾
        if PLOTLY_AVAILABLE:
            print(f"\nç”Ÿæˆäº¤äº’å¼3Då›¾...")
            fig_plotly = go.Figure()
            
            # å‚è€ƒäººç¾¤
            for pop in np.unique(reference_populations):
                mask = reference_populations == pop
                fig_plotly.add_trace(go.Scatter3d(
                    x=reference_pca[mask, 0],
                    y=reference_pca[mask, 1],
                    z=reference_pca[mask, 2],
                    mode='markers',
                    name=f'{pop} (å‚è€ƒ)',
                    marker=dict(
                        size=4,
                        color=pop_colors.get(pop, '#999999'),
                        opacity=0.4
                    ),
                    hovertemplate=f'{pop}<br>PC1: %{{x:.3f}}<br>PC2: %{{y:.3f}}<br>PC3: %{{z:.3f}}<extra></extra>'
                ))
            
            # æŸ¥è¯¢æ ·æœ¬
            for pop in np.unique(predictions):
                mask = predictions == pop
                samples_subset = query_samples[mask]
                fig_plotly.add_trace(go.Scatter3d(
                    x=query_pca[mask, 0],
                    y=query_pca[mask, 1],
                    z=query_pca[mask, 2],
                    mode='markers',
                    name=f'{pop} (æŸ¥è¯¢)',
                    marker=dict(
                        size=8,
                        color=pop_colors.get(pop, '#999999'),
                        opacity=0.9,
                        symbol='diamond',
                        line=dict(color='black', width=1)
                    ),
                    text=samples_subset,
                    hovertemplate='<b>%{text}</b><br>é¢„æµ‹: ' + pop + '<br>PC1: %{x:.3f}<br>PC2: %{y:.3f}<br>PC3: %{z:.3f}<extra></extra>'
                ))
            
            fig_plotly.update_layout(
                title='PCAæŠ•å½± - äº¤äº’å¼3Då›¾',
                scene=dict(
                    xaxis=dict(title='PC1'),
                    yaxis=dict(title='PC2'),
                    zaxis=dict(title='PC3')
                ),
                width=1400,
                height=1000
            )
            
            output_file = f'{output_prefix}_3d_interactive.html'
            fig_plotly.write_html(output_file)
            print(f"  ä¿å­˜: {output_file}")


def process_vcf_worker(args):
    """
    å¤šè¿›ç¨‹Workerå‡½æ•° - å¤„ç†å•ä¸ªVCFæ–‡ä»¶
    
    å‚æ•°:
        args: å…ƒç»„ï¼ŒåŒ…å«æ‰€æœ‰process_single_vcféœ€è¦çš„å‚æ•°
    
    è¿”å›:
        (vcf_name, simple_results, success, error_msg)
    """
    (vcf_file, output_path, snps_file, model_file, reference_csv,
     reference_pca, reference_populations, pca_model, mean_vals, 
     std_vals, positions, pop_labels, region, n_neighbors) = args
    
    vcf_basename = vcf_file.name.split('.')[0].split('-')[0].split('_')[0]
    output_subdir = output_path / vcf_basename
    output_subdir.mkdir(parents=True, exist_ok=True)
    
    try:
        simple_results, query_df = process_single_vcf(
            snps_file, model_file, reference_csv, str(vcf_file),
            output_subdir, reference_pca, reference_populations,
            pca_model, mean_vals, std_vals, positions, pop_labels,
            region, n_neighbors
        )
        return (vcf_file.name, simple_results, True, None)
    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        return (vcf_file.name, [], False, error_msg)


def process_single_vcf(snps_file, model_file, reference_csv, query_vcf,
                      output_subdir, reference_pca, reference_populations,
                      pca_model, mean_vals, std_vals, positions, pop_labels,
                      region=None, n_neighbors=20):
    """
    å¤„ç†å•ä¸ªVCFæ–‡ä»¶
    """
    vcf_basename = Path(query_vcf).name.split('.')[0].split('-')[0].split('_')[0]
    
    print(f"\n{'='*80}")
    print(f"å¤„ç†VCF: {Path(query_vcf).name}")
    print(f"{'='*80}")
    
    # æå–æŸ¥è¯¢æ ·æœ¬çš„åŸºå› å‹
    query_genotypes, query_samples, matched_positions = extract_genotypes_from_vcf(
        query_vcf, positions, region
    )
    
    # æŠ•å½±åˆ°PCAç©ºé—´
    query_pca = project_to_pca(query_genotypes, pca_model, mean_vals, std_vals)
    
    # æ¨æ–­ç§æ—
    predictions, probabilities, _ = infer_ancestry(
        query_pca, reference_pca, reference_populations, n_neighbors
    )
    
    # ä¿å­˜ç»“æœ
    print("\n" + "=" * 80)
    print(f"ä¿å­˜ç»“æœåˆ°: {output_subdir}")
    print("=" * 80)
    
    # å®Œæ•´CSVç»“æœ
    query_df = pd.DataFrame(query_pca, columns=[f'PC{i+1}' for i in range(query_pca.shape[1])])
    query_df['Sample'] = query_samples
    query_df['Predicted_Pop'] = predictions
    query_df['Confidence'] = np.max(probabilities, axis=1)
    
    for i, pop in enumerate(pop_labels):
        query_df[f'{pop}_prob'] = probabilities[:, i]
    
    csv_file = output_subdir / 'detailed_results.csv'
    query_df.to_csv(csv_file, index=False)
    print(f"  ä¿å­˜: {csv_file.name}")
    
    # ç®€æ´çš„é¢„æµ‹ç»“æœæ–‡ä»¶
    simple_results = []
    
    # ä»VCFæ–‡ä»¶åæå–æ ·æœ¬å
    vcf_file_path = Path(query_vcf)
    base_name = vcf_file_path.name.split('.vcf')[0]  # å»é™¤.vcf.gzæˆ–.vcfæ‰©å±•å
    # è¿›ä¸€æ­¥ç®€åŒ–ï¼šå–ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦ä¹‹å‰çš„éƒ¨åˆ†
    simple_vcf_name = base_name.split('.')[0].split('-')[0].split('_')[0]
    
    # å¦‚æœVCFåªæœ‰1ä¸ªæ ·æœ¬ï¼Œç›´æ¥ç”¨æ–‡ä»¶åï¼›å¦‚æœæœ‰å¤šä¸ªæ ·æœ¬ï¼Œç”¨"æ–‡ä»¶å_æ ·æœ¬åºå·"
    if len(query_samples) == 1:
        simple_results.append((simple_vcf_name, predictions[0]))
    else:
        for idx, (sample, pred) in enumerate(zip(query_samples, predictions), 1):
            # å¤šæ ·æœ¬VCFï¼šä½¿ç”¨"æ–‡ä»¶å_æ ·æœ¬åºå·"æ ¼å¼
            simple_name = f"{simple_vcf_name}_sample{idx}"
            simple_results.append((simple_name, pred))
    
    simple_file = output_subdir / f'{vcf_basename}_predictions.txt'
    with open(simple_file, 'w') as f:
        f.write("Sample\tPredicted_Population\n")
        for name, pred in simple_results:
            f.write(f"{name}\t{pred}\n")
    print(f"  ä¿å­˜: {simple_file.name}")
    
    # è¯¦ç»†æŠ¥å‘Š
    report_file = output_subdir / 'ancestry_report.txt'
    generate_ancestry_report(query_samples, predictions, probabilities, pop_labels, report_file)
    
    # å¯è§†åŒ–
    plot_combined_pca(
        reference_pca, reference_populations,
        query_pca, query_samples, predictions, probabilities,
        str(output_subdir / 'pca')
    )
    
    return simple_results, query_df


def main(snps_file, model_file, reference_csv, query_vcf_or_dir, 
         output_dir='projection_output', region=None, n_neighbors=20, n_jobs=1):
    """
    ä¸»å‡½æ•° - æ”¯æŒå•ä¸ªVCFæˆ–VCFæ–‡ä»¶å¤¹
    
    å‚æ•°:
        query_vcf_or_dir: VCFæ–‡ä»¶è·¯å¾„ æˆ– åŒ…å«VCFæ–‡ä»¶çš„æ–‡ä»¶å¤¹
        output_dir: è¾“å‡ºæ–‡ä»¶å¤¹
        n_jobs: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤1ï¼Œå•è¿›ç¨‹ï¼‰
    """
    print("\n" + "=" * 80)
    print("PCAæŠ•å½±ä¸ç¥–æºæ¨æ–­")
    print("=" * 80)
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_path.absolute()}")
    
    # æ£€æµ‹è¾“å…¥æ˜¯æ–‡ä»¶è¿˜æ˜¯æ–‡ä»¶å¤¹
    input_path = Path(query_vcf_or_dir)
    
    if input_path.is_file():
        # å•ä¸ªVCFæ–‡ä»¶
        vcf_files = [input_path]
        print(f"\nğŸ“„ è¾“å…¥: å•ä¸ªVCFæ–‡ä»¶")
        print(f"   {input_path.name}")
    elif input_path.is_dir():
        # VCFæ–‡ä»¶å¤¹
        vcf_files = sorted(list(input_path.glob('*.vcf.gz')) + list(input_path.glob('*.vcf')))
        if len(vcf_files) == 0:
            raise ValueError(f"æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°VCFæ–‡ä»¶: {input_path}")
        print(f"\nğŸ“ è¾“å…¥: VCFæ–‡ä»¶å¤¹")
        print(f"   æ‰¾åˆ° {len(vcf_files)} ä¸ªVCFæ–‡ä»¶")
        for vcf in vcf_files:
            print(f"   - {vcf.name}")
    else:
        raise ValueError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
    
    # 1. åŠ è½½æ¨¡å‹å’ŒSNPä½ç½®ï¼ˆåªéœ€åŠ è½½ä¸€æ¬¡ï¼‰
    pca_model, mean_vals, std_vals, positions = load_model_and_snps(model_file, snps_file)
    
    # 2. åŠ è½½å‚è€ƒäººç¾¤PCAç»“æœï¼ˆåªéœ€åŠ è½½ä¸€æ¬¡ï¼‰
    print("\n" + "=" * 80)
    print("åŠ è½½å‚è€ƒäººç¾¤æ•°æ®...")
    print("=" * 80)
    print(f"\nğŸ“‚ è¯»å–: {reference_csv}")
    reference_df = pd.read_csv(reference_csv)
    
    # æå–PCAåæ ‡å’Œç§æ—ä¿¡æ¯
    pc_cols = [col for col in reference_df.columns if col.startswith('PC')]
    reference_pca = reference_df[pc_cols].values
    reference_populations = reference_df['super_pop'].values if 'super_pop' in reference_df.columns else None
    
    if reference_populations is None:
        raise ValueError("å‚è€ƒæ•°æ®ç¼ºå°‘'super_pop'åˆ—")
    
    print(f"  å‚è€ƒæ ·æœ¬æ•°: {len(reference_df)}")
    print(f"  ä¸»æˆåˆ†æ•°: {len(pc_cols)}")
    print(f"  äººç¾¤åˆ†å¸ƒ:")
    for pop, count in reference_df['super_pop'].value_counts().items():
        print(f"    {pop}: {count}")
    
    # æ£€æŸ¥å¹¶è¿‡æ»¤ç¼ºå¤±å€¼
    has_missing = reference_df['super_pop'].isna().sum()
    if has_missing > 0:
        print(f"  âš ï¸  è­¦å‘Š: {has_missing} ä¸ªæ ·æœ¬ç¼ºå°‘äººç¾¤ä¿¡æ¯ï¼Œå·²è¿‡æ»¤")
        # åªä½¿ç”¨æœ‰äººç¾¤ä¿¡æ¯çš„æ ·æœ¬
        valid_mask = reference_df['super_pop'].notna()
        reference_df_filtered = reference_df[valid_mask].copy()
        reference_pca = reference_df_filtered[pc_cols].values
        reference_populations = reference_df_filtered['super_pop'].values
    
    # è·å–äººç¾¤æ ‡ç­¾ï¼ˆè¿‡æ»¤NaNåï¼‰
    pop_labels = sorted(reference_df['super_pop'].dropna().unique())
    
    # 3. å¤„ç†æ‰€æœ‰VCFæ–‡ä»¶
    all_predictions = []  # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
    
    print(f"\n" + "=" * 80)
    print(f"å¼€å§‹å¤„ç† {len(vcf_files)} ä¸ªVCFæ–‡ä»¶...")
    if n_jobs > 1 and len(vcf_files) > 1:
        print(f"ğŸš€ ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ (è¿›ç¨‹æ•°: {n_jobs})")
    print("=" * 80)
    
    if n_jobs > 1 and len(vcf_files) > 1:
        # å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
        print(f"\nå‡†å¤‡ {len(vcf_files)} ä¸ªä»»åŠ¡...")
        
        # å‡†å¤‡workerå‚æ•°
        worker_args = [
            (vcf_file, output_path, snps_file, model_file, reference_csv,
             reference_pca, reference_populations, pca_model, mean_vals,
             std_vals, positions, pop_labels, region, n_neighbors)
            for vcf_file in vcf_files
        ]
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†
        print(f"å¯åŠ¨ {n_jobs} ä¸ªè¿›ç¨‹...")
        with Pool(processes=n_jobs) as pool:
            # ä½¿ç”¨imap_unorderedä»¥ä¾¿åŠæ—¶è·å–ç»“æœ
            results = []
            for result in tqdm(pool.imap_unordered(process_vcf_worker, worker_args),
                             total=len(vcf_files), desc="æ€»ä½“è¿›åº¦"):
                results.append(result)
        
        # å¤„ç†ç»“æœ
        success_count = 0
        fail_count = 0
        
        print(f"\n" + "=" * 80)
        print("å¤„ç†ç»“æœ:")
        print("=" * 80)
        
        for vcf_name, simple_results, success, error_msg in results:
            if success:
                all_predictions.extend(simple_results)
                success_count += 1
                print(f"  âœ… {vcf_name}: {len(simple_results)} ä¸ªæ ·æœ¬")
            else:
                fail_count += 1
                print(f"  âŒ {vcf_name}: {error_msg}")
        
        print(f"\næ€»è®¡: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        
    else:
        # å•è¿›ç¨‹é¡ºåºå¤„ç†
        if n_jobs > 1:
            print(f"  â„¹ï¸  åªæœ‰1ä¸ªVCFæ–‡ä»¶ï¼Œä½¿ç”¨å•è¿›ç¨‹å¤„ç†")
        
        for i, vcf_file in enumerate(vcf_files, 1):
            print(f"\n[{i}/{len(vcf_files)}] {vcf_file.name}")
            
            # ä¸ºæ¯ä¸ªVCFåˆ›å»ºå­æ–‡ä»¶å¤¹
            vcf_basename = vcf_file.name.split('.')[0].split('-')[0].split('_')[0]
            output_subdir = output_path / vcf_basename
            output_subdir.mkdir(parents=True, exist_ok=True)
            
            try:
                # å¤„ç†å•ä¸ªVCF
                simple_results, query_df = process_single_vcf(
                    snps_file, model_file, reference_csv, str(vcf_file),
                    output_subdir, reference_pca, reference_populations,
                    pca_model, mean_vals, std_vals, positions, pop_labels,
                    region, n_neighbors
                )
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_predictions.extend(simple_results)
                
                print(f"  âœ… å®Œæˆ: {len(simple_results)} ä¸ªæ ·æœ¬")
                
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                traceback.print_exc()
                continue
    
    # 4. ç”Ÿæˆæ€»ç»“æœæ–‡ä»¶
    if len(all_predictions) > 0:
        print("\n" + "=" * 80)
        print("ç”Ÿæˆæ€»ç»“æœæ–‡ä»¶...")
        print("=" * 80)
        
        summary_file = output_path / 'all_samples_predictions.txt'
        with open(summary_file, 'w') as f:
            f.write("Sample\tPredicted_Population\n")
            for name, pred in all_predictions:
                f.write(f"{name}\t{pred}\n")
        
        print(f"\nğŸ“„ æ€»ç»“æœæ–‡ä»¶: {summary_file.name}")
        print(f"   åŒ…å« {len(all_predictions)} ä¸ªæ ·æœ¬")
        
        # ç»Ÿè®¡å„äººç¾¤æ•°é‡
        from collections import Counter
        pop_counts = Counter([pred for _, pred in all_predictions])
        print(f"\näººç¾¤åˆ†å¸ƒç»Ÿè®¡:")
        for pop in sorted(pop_counts.keys()):
            count = pop_counts[pop]
            print(f"  {pop}: {count} ({count/len(all_predictions)*100:.1f}%)")
    
    # 5. æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 80)
    print("åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_path.absolute()}")
    print(f"\næ ¸å¿ƒæ–‡ä»¶:")
    print(f"  ğŸ¯ all_samples_predictions.txt - æ€»ç»“æœæ–‡ä»¶ â­â­â­")
    print(f"     (åŒ…å«æ‰€æœ‰{len(all_predictions)}ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ)")
    
    if len(vcf_files) > 1:
        print(f"\nå„VCFå­æ–‡ä»¶å¤¹:")
        for vcf_file in vcf_files:
            vcf_basename = vcf_file.name.split('.')[0].split('-')[0].split('_')[0]
            print(f"  ğŸ“‚ {vcf_basename}/")
            print(f"     - {vcf_basename}_predictions.txt")
            print(f"     - detailed_results.csv")
            print(f"     - ancestry_report.txt")
            print(f"     - pca_2d.png, pca_3d.png")
    
    print(f"\nğŸ’¡ å¿«é€ŸæŸ¥çœ‹æ€»ç»“æœ:")
    print(f"   cat {output_path}/all_samples_predictions.txt")
    print(f"\nğŸ’¡ ç»Ÿè®¡å„äººç¾¤æ•°é‡:")
    print(f"   cut -f2 {output_path}/all_samples_predictions.txt | tail -n +2 | sort | uniq -c")
    
    return all_predictions


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='PCAæŠ•å½±ä¸ç¥–æºæ¨æ–­ - å°†æ–°æ ·æœ¬æŠ•å½±åˆ°å·²è®­ç»ƒçš„PCAç©ºé—´ï¼ˆæ”¯æŒå•æ–‡ä»¶æˆ–æ‰¹é‡å¤„ç†ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

  # 1. å¤„ç†å•ä¸ªVCFæ–‡ä»¶
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf new_cohort.vcf.gz \\
      --output-dir query_results
  
  # 2. æ‰¹é‡å¤„ç†VCFæ–‡ä»¶å¤¹ï¼ˆè‡ªåŠ¨å¤„ç†æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰VCFï¼‰ğŸ†•
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf vcf_folder/ \\
      --output-dir batch_results
  
  # 3. æ‰¹é‡å¤„ç†+å¤šè¿›ç¨‹åŠ é€Ÿï¼ˆæ¨èï¼‰ğŸš€
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf vcf_folder/ \\
      --output-dir batch_results \\
      --n-jobs 8
  
  # 4. æŒ‡å®šæŸ“è‰²ä½“åŒºåŸŸ
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf new_cohort.vcf.gz \\
      --region 20 \\
      --output-dir query_results
  
  # 5. è°ƒæ•´KNNé‚»å±…æ•°
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf new_cohort.vcf.gz \\
      --n-neighbors 30 \\
      --output-dir query_results

è¾“å‡ºæ–‡ä»¶ç»“æ„:

  å•ä¸ªVCFæ–‡ä»¶:
    output_dir/
    â”œâ”€â”€ all_samples_predictions.txt   # æ€»ç»“æœæ–‡ä»¶ â­â­â­
    â”œâ”€â”€ <vcf>/                        # VCFå­æ–‡ä»¶å¤¹
    â”‚   â”œâ”€â”€ <vcf>_predictions.txt     # è¯¥VCFçš„é¢„æµ‹ç»“æœ
    â”‚   â”œâ”€â”€ detailed_results.csv      # å®Œæ•´ç»“æœï¼ˆPCAåæ ‡+æ¦‚ç‡ï¼‰
    â”‚   â”œâ”€â”€ ancestry_report.txt       # è¯¦ç»†æŠ¥å‘Š
    â”‚   â”œâ”€â”€ pca_2d.png               # 2Då¯è§†åŒ–
    â”‚   â”œâ”€â”€ pca_3d.png               # 3Då¯è§†åŒ–
    â”‚   â””â”€â”€ pca_3d_interactive.html  # äº¤äº’å¼3Då›¾
  
  æ‰¹é‡å¤„ç†VCFæ–‡ä»¶å¤¹:
    output_dir/
    â”œâ”€â”€ all_samples_predictions.txt   # æ€»ç»“æœæ–‡ä»¶ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰â­â­â­
    â”œâ”€â”€ vcf1/                        # ç¬¬1ä¸ªVCFçš„ç»“æœ
    â”œâ”€â”€ vcf2/                        # ç¬¬2ä¸ªVCFçš„ç»“æœ
    â””â”€â”€ vcf3/                        # ç¬¬3ä¸ªVCFçš„ç»“æœ

å·¥ä½œæµç¨‹:
  1. è®­ç»ƒPCAæ¨¡å‹ï¼ˆä½¿ç”¨1000genomes_pca_ultimate.pyï¼‰
     python3 1000genomes_pca_ultimate.py ... --save-snps snps.npz --save-model model.pkl
  
  2. æŠ•å½±æ–°æ ·æœ¬ï¼ˆä½¿ç”¨æœ¬è„šæœ¬ï¼‰
     python3 pca_projection.py --snps snps.npz --model model.pkl --reference ref.csv --query-vcf new.vcf.gz
        """
    )
    
    parser.add_argument('--snps', required=True,
                       help='SNPä½ç½®åˆ—è¡¨æ–‡ä»¶ï¼ˆ.npzæ ¼å¼ï¼‰')
    parser.add_argument('--model', required=True,
                       help='PCAæ¨¡å‹æ–‡ä»¶ï¼ˆ.pklæ ¼å¼ï¼‰')
    parser.add_argument('--reference', required=True,
                       help='å‚è€ƒäººç¾¤PCAç»“æœCSVæ–‡ä»¶')
    parser.add_argument('--query-vcf', required=True,
                       help='æŸ¥è¯¢æ ·æœ¬çš„VCFæ–‡ä»¶ æˆ– åŒ…å«VCFæ–‡ä»¶çš„æ–‡ä»¶å¤¹ï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰')
    parser.add_argument('-o', '--output-dir', default='projection_output',
                       help='è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆé»˜è®¤: projection_outputï¼‰')
    parser.add_argument('-r', '--region',
                       help='æŸ“è‰²ä½“åŒºåŸŸï¼ˆå¯é€‰ï¼Œå¦‚: 20 æˆ– 20:1000000-2000000ï¼‰')
    parser.add_argument('--n-neighbors', type=int, default=20,
                       help='KNNåˆ†ç±»å™¨çš„é‚»å±…æ•°ï¼ˆé»˜è®¤: 20ï¼‰')
    parser.add_argument('--n-jobs', type=int, default=1,
                       help='å¹¶è¡Œå¤„ç†çš„è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: 1ï¼‰ã€‚å¤šä¸ªVCFæ–‡ä»¶æ—¶å»ºè®®è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°ï¼Œå¦‚ --n-jobs 8')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    for file, name in [(args.snps, 'SNPæ–‡ä»¶'), (args.model, 'æ¨¡å‹æ–‡ä»¶'), 
                       (args.reference, 'å‚è€ƒCSV'), (args.query_vcf, 'æŸ¥è¯¢VCF')]:
        if not os.path.exists(file):
            print(f"é”™è¯¯: {name}ä¸å­˜åœ¨: {file}")
            exit(1)
    
    try:
        main(
            snps_file=args.snps,
            model_file=args.model,
            reference_csv=args.reference,
            query_vcf_or_dir=args.query_vcf,
            output_dir=args.output_dir,
            region=args.region,
            n_neighbors=args.n_neighbors,
            n_jobs=args.n_jobs
        )
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
