#!/usr/bin/env python3
"""
PCA Projection Script - Project new samples onto trained PCA space

For ancestry inference: project samples with unknown ancestry onto 1000 Genomes PCA space,
and infer their ancestry.
"""

import numpy as np
import pandas as pd
import allel
import pickle
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import os
from pathlib import Path
from tqdm import tqdm
from multiprocessing import Pool
import traceback

# Try to import plotly
try:
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

# Set font for plots
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False


def load_model_and_snps(model_file, snps_file):
    """
    åŠ è½½PCAæ¨¡å‹å’ŒSNPä½ç½®åˆ—è¡¨
    """
    print("=" * 80)
    print("Loading PCA model and SNP positions...")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“‚ Loading PCA model: {model_file}")
    with open(model_file, 'rb') as f:
        model_data = pickle.load(f)
    
    pca_model = model_data['pca_model']
    mean_vals = model_data['mean']
    std_vals = model_data['std']
    n_components = model_data['n_components']
    n_features = model_data['n_features']
    
    print(f"  âœ“ Number of PCs: {n_components}")
    print(f"  âœ“ Number of features: {n_features}")
    print(f"  âœ“ Variance explained: {sum(pca_model.explained_variance_ratio_)*100:.2f}%")
    
    # åŠ è½½SNPä½ç½®
    print(f"\nğŸ“‚ Loading SNP position list: {snps_file}")
    snp_data = np.load(snps_file)
    positions = snp_data['positions']
    
    print(f"  âœ“ Number of SNPs: {len(positions)}")
    
    # æ£€æŸ¥positionså’Œn_featuresæ˜¯å¦ä¸€è‡´
    if len(positions) != n_features:
        print(f"\nâŒ Error: Number of SNP positions ({len(positions)}) does not match model features ({n_features}) ")
        print(f"\nPossible reasons:")
        print(f"  1. 1. Model files generated by old version of training script")
        print(f"  2. 2. snps.npz and model.pkl are from different training runs")
        print(f"\nSolution:")
        print(f"  Re-train using the latest version of training script (bug fixed):")
        print(f"  python3 1000genomes_pca_ultimate.py ... \\")
        print(f"      --save-snps snps.npz --save-model model.pkl")
        
        raise ValueError(f"SNP position list does not match model, please re-train")
    
    return pca_model, mean_vals, std_vals, positions


def normalize_chrom_name(chrom):
    """Normalize chromosome name, remove 'chr' prefix"""
    chrom_str = str(chrom)
    if chrom_str.startswith('chr'):
        return chrom_str[3:]
    return chrom_str


def decode_position_with_chrom(position_offset):
    """
    Decode chromosome and actual position from offset-encoded position
    
    è®­ç»ƒæ—¶ç¼–ç è§„åˆ™ï¼š
    position_offset = chromosome * 1_000_000_000 + actual_position
    """
    chrom = int(position_offset // 1_000_000_000)
    actual_pos = int(position_offset % 1_000_000_000)
    return chrom, actual_pos


def extract_genotypes_from_vcf(vcf_file, positions, region=None):
    """
    ä»VCFæ–‡ä»¶æå–æŒ‡å®šä½ç½®çš„åŸºå› å‹
    æ”¯æŒè‡ªåŠ¨å¤„ç†æŸ“è‰²ä½“å‘½åå·®å¼‚ï¼ˆchr1 vs 1ï¼‰
    
    å‚æ•°:
        vcf_file: VCFæ–‡ä»¶è·¯å¾„
        positions: éœ€è¦æå–çš„ä½ç½®åˆ—è¡¨ï¼ˆå·²ç¼–ç offsetï¼‰
        region: æŸ“è‰²ä½“åŒºåŸŸï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        genotypes: åŸºå› å‹çŸ©é˜µ (samples x variants)
        samples: æ ·æœ¬IDåˆ—è¡¨
        matched_positions: å®é™…åŒ¹é…åˆ°çš„ä½ç½®
    """
    print("\n" + "=" * 80)
    print("Extracting genotype data from VCF...")
    print("=" * 80)
    
    print(f"\nğŸ“‚ Reading VCF: {vcf_file}")
    if region:
        print(f"  åŒºåŸŸ: {region}")
    
    # è¯»å–VCF
    try:
        callset = allel.read_vcf(vcf_file, region=region)
    except Exception as e:
        raise ValueError(f"è¯»å–VCFå¤±è´¥: {e}")
    
    if callset is None or 'calldata/GT' not in callset:
        raise ValueError("VCFæ–‡ä»¶æ— æ•°æ®æˆ–ç¼ºå°‘åŸºå› å‹ä¿¡æ¯")
    
    # è·å–VCFä¸­çš„æŸ“è‰²ä½“å’Œä½ç½®
    vcf_chroms = callset['variants/CHROM']
    vcf_positions = callset['variants/POS'].astype(np.int64)
    genotypes = allel.GenotypeArray(callset['calldata/GT'])
    samples = callset.get('samples', None)
    
    if samples is None:
        raise ValueError("VCFæ–‡ä»¶ç¼ºå°‘æ ·æœ¬ä¿¡æ¯")
    
    # ç¡®ä¿samplesæ˜¯å­—ç¬¦ä¸²
    if isinstance(samples[0], bytes):
        samples = np.array([s.decode('utf-8') for s in samples])
    else:
        samples = np.array([str(s) for s in samples])
    
    print(f"  Number of samples in VCF: {len(samples)}")
    print(f"  Number of variants in VCF: {len(vcf_positions)}")
    
    # æ£€æµ‹VCFæŸ“è‰²ä½“å‘½åæ ¼å¼
    first_chrom = vcf_chroms[0].decode('utf-8') if isinstance(vcf_chroms[0], bytes) else str(vcf_chroms[0])
    vcf_has_chr_prefix = first_chrom.startswith('chr')
    
    print(f"  æŸ“è‰²ä½“å‘½åæ ¼å¼: {first_chrom} ({'æœ‰chrå‰ç¼€' if vcf_has_chr_prefix else 'æ— chrå‰ç¼€'})")
    
    # æ£€æµ‹Referenceä½ç½®çš„æ ¼å¼ï¼ˆä»ç¬¬ä¸€ä¸ªä½ç‚¹è§£ç ï¼‰
    ref_chrom_example, ref_pos_example = decode_position_with_chrom(positions[0])
    print(f"  Referenceæ•°æ®æ ¼å¼: æŸ“è‰²ä½“{ref_chrom_example} (æ— chrå‰ç¼€)")
    
    # å†³å®šæ˜¯å¦éœ€è¦è½¬æ¢
    need_normalize = vcf_has_chr_prefix
    
    if need_normalize:
        print(f"  ğŸ”„ è‡ªåŠ¨è½¬æ¢: å°†VCFçš„'chr{ref_chrom_example}'æ ¼å¼è½¬ä¸º'{ref_chrom_example}'æ ¼å¼")
    else:
        print(f"  âœ“ å‘½åæ ¼å¼ä¸€è‡´ï¼Œæ— éœ€è½¬æ¢")
    
    # æ ‡å‡†åŒ–VCFçš„æŸ“è‰²ä½“åç§°ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if need_normalize:
        if isinstance(vcf_chroms[0], bytes):
            vcf_chroms_normalized = np.array([normalize_chrom_name(c.decode('utf-8')) for c in vcf_chroms])
        else:
            vcf_chroms_normalized = np.array([normalize_chrom_name(c) for c in vcf_chroms])
    else:
        if isinstance(vcf_chroms[0], bytes):
            vcf_chroms_normalized = np.array([c.decode('utf-8') for c in vcf_chroms])
        else:
            vcf_chroms_normalized = np.array([str(c) for c in vcf_chroms])
    
    # åˆ›å»ºVCFçš„æŸ“è‰²ä½“+ä½ç½®ç´¢å¼•
    vcf_chrom_pos_dict = {}
    for i, (chrom, pos) in enumerate(zip(vcf_chroms_normalized, vcf_positions)):
        key = (chrom, pos)
        vcf_chrom_pos_dict[key] = i
    
    # åŒ¹é…ä½ç½®
    print(f"\nğŸ” Matching SNP positions...")
    print(f"  Reference sites: {len(positions)}")
    
    matched_indices = []
    matched_positions = []
    
    print(f"  æ­£åœ¨åŒ¹é…ä½ç‚¹ï¼ˆè‡ªåŠ¨å¤„ç†æŸ“è‰²ä½“å‘½åå·®å¼‚ï¼‰...")
    for i, pos_offset in tqdm(enumerate(positions), total=len(positions), desc="  åŒ¹é…è¿›åº¦"):
        # ä»offsetè§£ç å‡ºæŸ“è‰²ä½“å’Œå®é™…ä½ç½®
        chrom, actual_pos = decode_position_with_chrom(pos_offset)
        chrom_str = str(chrom)
        
        # åœ¨VCFä¸­æŸ¥æ‰¾
        key = (chrom_str, actual_pos)
        if key in vcf_chrom_pos_dict:
            vcf_idx = vcf_chrom_pos_dict[key]
            matched_indices.append((i, vcf_idx))
            matched_positions.append(pos_offset)
    
    n_matched = len(matched_indices)
    match_rate = n_matched / len(positions)
    
    print(f"  Matched sites: {n_matched}")
    print(f"  Match rate: {match_rate*100:.1f}%")
    
    if match_rate < 0.5:
        print(f"\nâš ï¸  Warning: Match rate too low ({match_rate*100:.1f}%)")
        print(f"  Suggestion: Check if VCF uses the same reference genome")
    elif match_rate < 0.8:
        print(f"\nâš ï¸  æ³¨æ„: åŒ¹é…ç‡è¾ƒä½ ({match_rate*100:.1f}%)ï¼Œç»“æœå¯èƒ½ä¸å¤Ÿå‡†ç¡®")
    
    if n_matched == 0:
        raise ValueError("No matching SNP sites! Please check VCF file and reference SNP list")
    
    # æå–åŒ¹é…ä½ç‚¹çš„åŸºå› å‹
    print(f"\nğŸ“Š æå–åŸºå› å‹æ•°æ®...")
    ref_indices = [idx[0] for idx in matched_indices]
    vcf_indices = [idx[1] for idx in matched_indices]
    
    # åˆ›å»ºå®Œæ•´çš„åŸºå› å‹çŸ©é˜µï¼ˆåŒ…å«ç¼ºå¤±ä½ç‚¹ï¼‰
    n_samples = len(samples)
    n_ref_snps = len(positions)
    full_genotypes = np.full((n_samples, n_ref_snps), -1, dtype=np.int8)
    
    # å¡«å……åŒ¹é…çš„ä½ç‚¹
    matched_genotypes = genotypes[vcf_indices].to_n_alt().T
    for i, ref_idx in enumerate(ref_indices):
        full_genotypes[:, ref_idx] = matched_genotypes[:, i]
    
    print(f"  æå–Completed: {n_samples} æ ·æœ¬ Ã— {n_ref_snps} ä½ç‚¹")
    print(f"  ç¼ºå¤±ä½ç‚¹æ•°: {n_ref_snps - n_matched}")
    
    return full_genotypes, samples, np.array(matched_positions)


def project_to_pca(genotypes, pca_model, mean_vals, std_vals):
    """
    å°†æ–°æ ·æœ¬æŠ•å½±åˆ°PCAç©ºé—´
    
    å‚æ•°:
        genotypes: åŸºå› å‹çŸ©é˜µ (samples x variants)
        pca_model: è®­ç»ƒå¥½çš„PCAæ¨¡å‹
        mean_vals: æ ‡å‡†åŒ–å‡å€¼
        std_vals: æ ‡å‡†åŒ–æ ‡å‡†å·®
    
    è¿”å›:
        pca_coords: PCAåæ ‡ (samples x n_components)
    """
    print("\n" + "=" * 80)
    print("Projecting to PCA space...")
    print("=" * 80)
    
    n_samples, n_variants = genotypes.shape
    print(f"\nè¾“å…¥: {n_samples} æ ·æœ¬ Ã— {n_variants} å˜å¼‚")
    
    # Handle missing valuesï¼ˆç”¨å‡å€¼å¡«å……ï¼‰
    print(f"\nHandling missing values...")
    genotypes_clean = genotypes.copy().astype(float)
    
    missing_count = np.sum(genotypes_clean == -1)
    missing_rate = missing_count / (n_samples * n_variants)
    print(f"  Number of missing values: {missing_count}")
    print(f"  Missing rate: {missing_rate*100:.2f}%")
    
    print(f"  Imputing missing values...")
    for i in tqdm(range(n_variants), desc="  å¡«å……è¿›åº¦"):
        col = genotypes_clean[:, i]
        mask = col != -1
        if np.sum(mask) > 0:
            mean_val = np.mean(col[mask])
            col[~mask] = mean_val
        else:
            # å¦‚æœæ•´åˆ—éƒ½ç¼ºå¤±ï¼Œç”¨Referenceå‡å€¼å¡«å……
            col[:] = mean_vals[i]
    
    # åº”ç”¨ä¸Referenceäººç¾¤ç›¸åŒçš„æ ‡å‡†åŒ–
    print(f"\nApplying standardization...")
    genotypes_scaled = (genotypes_clean - mean_vals) / std_vals
    
    # æŠ•å½±
    print(f"\nExecuting PCA projection...")
    pca_coords = pca_model.transform(genotypes_scaled)
    
    print(f"  âœ“ æŠ•å½±å®Œæˆ")
    print(f"  è¾“å‡º: {pca_coords.shape[0]} æ ·æœ¬ Ã— {pca_coords.shape[1]} ä¸»æˆåˆ†")
    
    return pca_coords


def infer_ancestry(query_pca, reference_pca, reference_populations, n_neighbors=20):
    """
    ä½¿ç”¨KNNæ¨æ–­ç§æ—å½’å±
    
    å‚æ•°:
        query_pca: Queryæ ·æœ¬çš„PCAåæ ‡
        reference_pca: Referenceäººç¾¤çš„PCAåæ ‡
        reference_populations: Referenceäººç¾¤çš„ç§æ—æ ‡ç­¾
        n_neighbors: KNNçš„é‚»å±…æ•°
    
    è¿”å›:
        predictions: é¢„æµ‹çš„ç§æ—
        probabilities: å„ç§æ—çš„æ¦‚ç‡
    """
    print("\n" + "=" * 80)
    print("æ¨æ–­ç§æ—å½’å±...")
    print("=" * 80)
    
    print(f"\nä½¿ç”¨KNNåˆ†ç±»å™¨ (K={n_neighbors})")
    print(f"  Referenceæ ·æœ¬æ•°: {len(reference_pca)}")
    print(f"  Number of query samples: {len(query_pca)}")
    
    # è®­ç»ƒKNN
    knn = KNeighborsClassifier(n_neighbors=n_neighbors)
    knn.fit(reference_pca, reference_populations)
    
    # é¢„æµ‹
    predictions = knn.predict(query_pca)
    probabilities = knn.predict_proba(query_pca)
    
    # ç»Ÿè®¡
    unique_pops, counts = np.unique(predictions, return_counts=True)
    print(f"\né¢„æµ‹ç»“æœç»Ÿè®¡:")
    for pop, count in zip(unique_pops, counts):
        print(f"  {pop}: {count} æ ·æœ¬ ({count/len(predictions)*100:.1f}%)")
    
    return predictions, probabilities, knn.classes_


def generate_ancestry_report(samples, predictions, probabilities, pop_labels, output_file):
    """
    ç”Ÿæˆè¯¦ç»†çš„ç¥–æºæ¨æ–­æŠ¥å‘Š
    """
    print(f"\nç”Ÿæˆç¥–æºæ¨æ–­æŠ¥å‘Š: {output_file}")
    
    with open(output_file, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("ç¥–æºæ¨æ–­æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"æ ·æœ¬æ•°: {len(samples)}\n")
        f.write(f"Referenceäººç¾¤: {', '.join(pop_labels)}\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("è¯¦ç»†ç»“æœ\n")
        f.write("=" * 80 + "\n\n")
        
        for i, sample in enumerate(samples):
            pred_pop = predictions[i]
            probs = probabilities[i]
            confidence = np.max(probs)
            
            f.write(f"Sample: {sample}\n")
            f.write(f"  é¢„æµ‹ç§æ—: {pred_pop}\n")
            f.write(f"  Confidence: {confidence*100:.1f}%\n")
            f.write(f"  æ¦‚ç‡åˆ†å¸ƒ:\n")
            
            for pop, prob in zip(pop_labels, probs):
                f.write(f"    {pop}: {prob*100:.1f}%\n")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ··åˆäººç¾¤
            sorted_probs = sorted(zip(pop_labels, probs), key=lambda x: x[1], reverse=True)
            if sorted_probs[1][1] > 0.2:  # ç¬¬äºŒé«˜çš„æ¦‚ç‡>20%
                f.write(f"  æ³¨é‡Š: å¯èƒ½ä¸ºæ··åˆäººç¾¤ ({sorted_probs[0][0]}/{sorted_probs[1][0]})\n")
            
            f.write("\n")
    
    print(f"  âœ“ æŠ¥å‘Šå·²ä¿å­˜")


def plot_combined_pca(reference_pca, reference_populations, query_pca, 
                      query_samples, predictions, probabilities,
                      output_prefix='projection'):
    """
    ç»˜åˆ¶Referenceäººç¾¤å’ŒQueryæ ·æœ¬çš„ç»„åˆPCAå›¾
    """
    print("\n" + "=" * 80)
    print("ç”ŸæˆVisualizationå›¾è¡¨...")
    print("=" * 80)
    
    # Color definitions
    pop_colors = {
        'AFR': '#E74C3C',  # red
        'AMR': '#9B59B6',  # purple
        'EAS': '#3498DB',  # blue
        'EUR': '#F39C12',  # orange
        'SAS': '#2ECC71'   # green
    }
    
    # 2D plot
    print(f"\nç”Ÿæˆ2Dæ•£ç‚¹å›¾...")
    fig, ax = plt.subplots(figsize=(16, 12))
    
    # ç»˜åˆ¶Referenceäººç¾¤ï¼ˆå°åœ†ç‚¹ï¼‰
    for pop in np.unique(reference_populations):
        mask = reference_populations == pop
        ax.scatter(
            reference_pca[mask, 0],
            reference_pca[mask, 1],
            c=pop_colors.get(pop, '#999999'),
            label=f'{pop} (Reference)',
            alpha=0.4,
            s=30,
            marker='o'
        )
    
    # ç»˜åˆ¶Queryæ ·æœ¬ï¼ˆæ˜Ÿå·ï¼‰
    for pop in np.unique(predictions):
        mask = predictions == pop
        ax.scatter(
            query_pca[mask, 0],
            query_pca[mask, 1],
            c=pop_colors.get(pop, '#999999'),
            label=f'{pop} (Query)',
            alpha=0.9,
            s=150,
            marker='*',
            edgecolors='black',
            linewidths=1.5
        )
    
    ax.set_xlabel('PC1', fontsize=14, fontweight='bold')
    ax.set_ylabel('PC2', fontsize=14, fontweight='bold')
    ax.set_title('PCAæŠ•å½± - Referenceäººç¾¤ä¸Queryæ ·æœ¬', fontsize=16, fontweight='bold')
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_file = f'{output_prefix}_2d.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"  ä¿å­˜: {output_file}")
    plt.close()
    
    # 3Då›¾
    if reference_pca.shape[1] >= 3:
        print(f"\nç”Ÿæˆ3Dæ•£ç‚¹å›¾...")
        fig = plt.figure(figsize=(18, 14))
        ax = fig.add_subplot(111, projection='3d')
        
        # Referenceäººç¾¤
        for pop in np.unique(reference_populations):
            mask = reference_populations == pop
            ax.scatter(
                reference_pca[mask, 0],
                reference_pca[mask, 1],
                reference_pca[mask, 2],
                c=pop_colors.get(pop, '#999999'),
                label=f'{pop} (Reference)',
                alpha=0.3,
                s=30,
                marker='o'
            )
        
        # Queryæ ·æœ¬
        for pop in np.unique(predictions):
            mask = predictions == pop
            ax.scatter(
                query_pca[mask, 0],
                query_pca[mask, 1],
                query_pca[mask, 2],
                c=pop_colors.get(pop, '#999999'),
                label=f'{pop} (Query)',
                alpha=0.9,
                s=150,
                marker='*',
                edgecolors='black',
                linewidths=1.5
            )
        
        ax.set_xlabel('PC1', fontweight='bold')
        ax.set_ylabel('PC2', fontweight='bold')
        ax.set_zlabel('PC3', fontweight='bold')
        ax.set_title('PCA Projection - 3D View', fontsize=16, fontweight='bold')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        
        plt.tight_layout()
        output_file = f'{output_prefix}_3d.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"  ä¿å­˜: {output_file}")
        plt.close()
        
        # Interactive 3D plot
        if PLOTLY_AVAILABLE:
            print(f"\nGenerating interactive 3D plot...")
            fig_plotly = go.Figure()
            
            # Referenceäººç¾¤
            for pop in np.unique(reference_populations):
                mask = reference_populations == pop
                fig_plotly.add_trace(go.Scatter3d(
                    x=reference_pca[mask, 0],
                    y=reference_pca[mask, 1],
                    z=reference_pca[mask, 2],
                    mode='markers',
                    name=f'{pop} (Reference)',
                    marker=dict(
                        size=4,
                        color=pop_colors.get(pop, '#999999'),
                        opacity=0.4
                    ),
                    hovertemplate=f'{pop}<br>PC1: %{{x:.3f}}<br>PC2: %{{y:.3f}}<br>PC3: %{{z:.3f}}<extra></extra>'
                ))
            
            # Queryæ ·æœ¬
            for pop in np.unique(predictions):
                mask = predictions == pop
                samples_subset = query_samples[mask]
                fig_plotly.add_trace(go.Scatter3d(
                    x=query_pca[mask, 0],
                    y=query_pca[mask, 1],
                    z=query_pca[mask, 2],
                    mode='markers',
                    name=f'{pop} (Query)',
                    marker=dict(
                        size=8,
                        color=pop_colors.get(pop, '#999999'),
                        opacity=0.9,
                        symbol='diamond',
                        line=dict(color='black', width=1)
                    ),
                    text=samples_subset,
                    hovertemplate='<b>%{text}</b><br>é¢„æµ‹: ' + pop + '<br>PC1: %{x:.3f}<br>PC2: %{y:.3f}<br>PC3: %{z:.3f}<extra></extra>'
                ))
            
            fig_plotly.update_layout(
                title='PCA Projection - Interactive 3Då›¾',
                scene=dict(
                    xaxis=dict(title='PC1'),
                    yaxis=dict(title='PC2'),
                    zaxis=dict(title='PC3')
                ),
                width=1400,
                height=1000
            )
            
            output_file = f'{output_prefix}_3d_interactive.html'
            fig_plotly.write_html(output_file)
            print(f"  ä¿å­˜: {output_file}")


def process_vcf_worker(args):
    """
    å¤šè¿›ç¨‹Workerå‡½æ•° - Process single VCF file
    
    å‚æ•°:
        args: å…ƒç»„ï¼ŒåŒ…å«æ‰€æœ‰process_single_vcféœ€è¦çš„å‚æ•°
    
    è¿”å›:
        (vcf_name, simple_results, success, error_msg)
    """
    (vcf_file, output_path, snps_file, model_file, reference_csv,
     reference_pca, reference_populations, pca_model, mean_vals, 
     std_vals, positions, pop_labels, region, n_neighbors) = args
    
    vcf_basename = vcf_file.name.split('.')[0].split('-')[0].split('_')[0]
    output_subdir = output_path / vcf_basename
    output_subdir.mkdir(parents=True, exist_ok=True)
    
    try:
        simple_results, query_df = process_single_vcf(
            snps_file, model_file, reference_csv, str(vcf_file),
            output_subdir, reference_pca, reference_populations,
            pca_model, mean_vals, std_vals, positions, pop_labels,
            region, n_neighbors
        )
        return (vcf_file.name, simple_results, True, None)
    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        return (vcf_file.name, [], False, error_msg)


def process_single_vcf(snps_file, model_file, reference_csv, query_vcf,
                      output_subdir, reference_pca, reference_populations,
                      pca_model, mean_vals, std_vals, positions, pop_labels,
                      region=None, n_neighbors=20):
    """
    Process single VCF file
    """
    vcf_basename = Path(query_vcf).name.split('.')[0].split('-')[0].split('_')[0]
    
    print(f"\n{'='*80}")
    print(f"Processing VCF: {Path(query_vcf).name}")
    print(f"{'='*80}")
    
    # æå–Queryæ ·æœ¬çš„åŸºå› å‹
    query_genotypes, query_samples, matched_positions = extract_genotypes_from_vcf(
        query_vcf, positions, region
    )
    
    # æŠ•å½±åˆ°PCAç©ºé—´
    query_pca = project_to_pca(query_genotypes, pca_model, mean_vals, std_vals)
    
    # æ¨æ–­ç§æ—
    predictions, probabilities, _ = infer_ancestry(
        query_pca, reference_pca, reference_populations, n_neighbors
    )
    
    # ä¿å­˜ç»“æœ
    print("\n" + "=" * 80)
    print(f"Saving results to: {output_subdir}")
    print("=" * 80)
    
    # å®Œæ•´CSVç»“æœ
    query_df = pd.DataFrame(query_pca, columns=[f'PC{i+1}' for i in range(query_pca.shape[1])])
    query_df['Sample'] = query_samples
    query_df['Predicted_Pop'] = predictions
    query_df['Confidence'] = np.max(probabilities, axis=1)
    
    for i, pop in enumerate(pop_labels):
        query_df[f'{pop}_prob'] = probabilities[:, i]
    
    csv_file = output_subdir / 'detailed_results.csv'
    query_df.to_csv(csv_file, index=False)
    print(f"  ä¿å­˜: {csv_file.name}")
    
    # ç®€æ´çš„é¢„æµ‹ç»“æœæ–‡ä»¶
    simple_results = []
    
    # Extract sample name from VCF filename
    vcf_file_path = Path(query_vcf)
    base_name = vcf_file_path.name.split('.vcf')[0]  # Remove .vcf.gz or .vcf extension
    # Further simplify: take part before first separator
    simple_vcf_name = base_name.split('.')[0].split('-')[0].split('_')[0]
    
    # If VCF has 1 sample, use filename; if multiple samples, use "filename_sampleN"
    if len(query_samples) == 1:
        simple_results.append((simple_vcf_name, predictions[0]))
    else:
        for idx, (sample, pred) in enumerate(zip(query_samples, predictions), 1):
            # Multi-sample VCF: use "filename_sampleN" format
            simple_name = f"{simple_vcf_name}_sample{idx}"
            simple_results.append((simple_name, pred))
    
    simple_file = output_subdir / f'{vcf_basename}_predictions.txt'
    with open(simple_file, 'w') as f:
        f.write("Sample\tPredicted_Population\n")
        for name, pred in simple_results:
            f.write(f"{name}\t{pred}\n")
    print(f"  ä¿å­˜: {simple_file.name}")
    
    # Detailed report
    report_file = output_subdir / 'ancestry_report.txt'
    generate_ancestry_report(query_samples, predictions, probabilities, pop_labels, report_file)
    
    # Visualization
    plot_combined_pca(
        reference_pca, reference_populations,
        query_pca, query_samples, predictions, probabilities,
        str(output_subdir / 'pca')
    )
    
    return simple_results, query_df


def main(snps_file, model_file, reference_csv, query_vcf_or_dir, 
         output_dir='projection_output', region=None, n_neighbors=20, n_jobs=1):
    """
    ä¸»å‡½æ•° - æ”¯æŒå•ä¸ªVCFæˆ–VCFæ–‡ä»¶å¤¹
    
    å‚æ•°:
        query_vcf_or_dir: VCFæ–‡ä»¶è·¯å¾„ æˆ– åŒ…å«VCFæ–‡ä»¶çš„æ–‡ä»¶å¤¹
        output_dir: è¾“å‡ºæ–‡ä»¶å¤¹
        n_jobs: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤1ï¼Œå•è¿›ç¨‹ï¼‰
    """
    print("\n" + "=" * 80)
    print("PCA Projection and Ancestry Inference")
    print("=" * 80)
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    print(f"\nğŸ“ Output folder: {output_path.absolute()}")
    
    # æ£€æµ‹è¾“å…¥æ˜¯æ–‡ä»¶è¿˜æ˜¯æ–‡ä»¶å¤¹
    input_path = Path(query_vcf_or_dir)
    
    if input_path.is_file():
        # å• VCF files
        vcf_files = [input_path]
        print(f"\nğŸ“„ Input: Single VCF file")
        print(f"   {input_path.name}")
    elif input_path.is_dir():
        # VCFæ–‡ä»¶å¤¹
        vcf_files = sorted(list(input_path.glob('*.vcf.gz')) + list(input_path.glob('*.vcf')))
        if len(vcf_files) == 0:
            raise ValueError(f"No VCF files found in folder: {input_path}")
        print(f"\nğŸ“ Input: VCF folder")
        print(f"   Found {len(vcf_files)}  VCF files")
        for vcf in vcf_files:
            print(f"   - {vcf.name}")
    else:
        raise ValueError(f"Input path does not exist: {input_path}")
    
    # 1. åŠ è½½æ¨¡å‹å’ŒSNPä½ç½®ï¼ˆåªéœ€åŠ è½½ä¸€æ¬¡ï¼‰
    pca_model, mean_vals, std_vals, positions = load_model_and_snps(model_file, snps_file)
    
    # 2. åŠ è½½Referenceäººç¾¤PCAç»“æœï¼ˆåªéœ€åŠ è½½ä¸€æ¬¡ï¼‰
    print("\n" + "=" * 80)
    print("åŠ è½½Referenceäººç¾¤æ•°æ®...")
    print("=" * 80)
    print(f"\nğŸ“‚ Reading: {reference_csv}")
    reference_df = pd.read_csv(reference_csv)
    
    # æå–PCAåæ ‡å’Œç§æ—ä¿¡æ¯
    pc_cols = [col for col in reference_df.columns if col.startswith('PC')]
    reference_pca = reference_df[pc_cols].values
    reference_populations = reference_df['super_pop'].values if 'super_pop' in reference_df.columns else None
    
    if reference_populations is None:
        raise ValueError("Referenceæ•°æ®ç¼ºå°‘'super_pop'åˆ—")
    
    print(f"  Referenceæ ·æœ¬æ•°: {len(reference_df)}")
    print(f"  Number of PCs: {len(pc_cols)}")
    print(f"  Population distribution:")
    for pop, count in reference_df['super_pop'].value_counts().items():
        print(f"    {pop}: {count}")
    
    # æ£€æŸ¥å¹¶è¿‡æ»¤ç¼ºå¤±å€¼
    has_missing = reference_df['super_pop'].isna().sum()
    if has_missing > 0:
        print(f"  âš ï¸  Warning: {has_missing}  samples missing population info, filtered")
        # åªä½¿ç”¨æœ‰äººç¾¤ä¿¡æ¯çš„æ ·æœ¬
        valid_mask = reference_df['super_pop'].notna()
        reference_df_filtered = reference_df[valid_mask].copy()
        reference_pca = reference_df_filtered[pc_cols].values
        reference_populations = reference_df_filtered['super_pop'].values
    
    # è·å–äººç¾¤æ ‡ç­¾ï¼ˆè¿‡æ»¤NaNåï¼‰
    pop_labels = sorted(reference_df['super_pop'].dropna().unique())
    
    # 3. å¤„ç†æ‰€æœ‰VCFæ–‡ä»¶
    all_predictions = []  # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
    
    print(f"\n" + "=" * 80)
    print(f"Starting to process {len(vcf_files)}  VCF files...")
    if n_jobs > 1 and len(vcf_files) > 1:
        print(f"ğŸš€ Multi-processing enabled (processes: {n_jobs})")
    print("=" * 80)
    
    if n_jobs > 1 and len(vcf_files) > 1:
        # å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
        print(f"\nPreparing {len(vcf_files)}  tasks...")
        
        # å‡†å¤‡workerå‚æ•°
        worker_args = [
            (vcf_file, output_path, snps_file, model_file, reference_csv,
             reference_pca, reference_populations, pca_model, mean_vals,
             std_vals, positions, pop_labels, region, n_neighbors)
            for vcf_file in vcf_files
        ]
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†
        print(f"Starting {n_jobs}  processes...")
        with Pool(processes=n_jobs) as pool:
            # ä½¿ç”¨imap_unorderedä»¥ä¾¿åŠæ—¶è·å–ç»“æœ
            results = []
            for result in tqdm(pool.imap_unordered(process_vcf_worker, worker_args),
                             total=len(vcf_files), desc="Overall progress"):
                results.append(result)
        
        # å¤„ç†ç»“æœ
        success_count = 0
        fail_count = 0
        
        print(f"\n" + "=" * 80)
        print("Processing results:")
        print("=" * 80)
        
        for vcf_name, simple_results, success, error_msg in results:
            if success:
                all_predictions.extend(simple_results)
                success_count += 1
                print(f"  âœ… {vcf_name}: {len(simple_results)} ä¸ªæ ·æœ¬")
            else:
                fail_count += 1
                print(f"  âŒ {vcf_name}: {error_msg}")
        
        print(f"\nTotal: success {success_count}, failed {fail_count}")
        
    else:
        # å•è¿›ç¨‹é¡ºåºå¤„ç†
        if n_jobs > 1:
            print(f"  â„¹ï¸  åªæœ‰1 VCF filesï¼Œä½¿ç”¨å•è¿›ç¨‹å¤„ç†")
        
        for i, vcf_file in enumerate(vcf_files, 1):
            print(f"\n[{i}/{len(vcf_files)}] {vcf_file.name}")
            
            # ä¸ºæ¯ä¸ªVCFåˆ›å»ºå­æ–‡ä»¶å¤¹
            vcf_basename = vcf_file.name.split('.')[0].split('-')[0].split('_')[0]
            output_subdir = output_path / vcf_basename
            output_subdir.mkdir(parents=True, exist_ok=True)
            
            try:
                # å¤„ç†å•ä¸ªVCF
                simple_results, query_df = process_single_vcf(
                    snps_file, model_file, reference_csv, str(vcf_file),
                    output_subdir, reference_pca, reference_populations,
                    pca_model, mean_vals, std_vals, positions, pop_labels,
                    region, n_neighbors
                )
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_predictions.extend(simple_results)
                
                print(f"  âœ… Completed: {len(simple_results)} ä¸ªæ ·æœ¬")
                
            except Exception as e:
                print(f"  âŒ Processing failed: {e}")
                traceback.print_exc()
                continue
    
    # 4. ç”ŸæˆSummary file
    if len(all_predictions) > 0:
        print("\n" + "=" * 80)
        print("Generating summary file...")
        print("=" * 80)
        
        summary_file = output_path / 'all_samples_predictions.txt'
        with open(summary_file, 'w') as f:
            f.write("Sample\tPredicted_Population\n")
            for name, pred in all_predictions:
                f.write(f"{name}\t{pred}\n")
        
        print(f"\nğŸ“„ Summary file: {summary_file.name}")
        print(f"   åŒ…å« {len(all_predictions)} ä¸ªæ ·æœ¬")
        
        # ç»Ÿè®¡å„äººç¾¤æ•°é‡
        from collections import Counter
        pop_counts = Counter([pred for _, pred in all_predictions])
        print(f"\näººç¾¤åˆ†å¸ƒç»Ÿè®¡:")
        for pop in sorted(pop_counts.keys()):
            count = pop_counts[pop]
            print(f"  {pop}: {count} ({count/len(all_predictions)*100:.1f}%)")
    
    # 5. æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 80)
    print("åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“ Output folder: {output_path.absolute()}")
    print(f"\næ ¸å¿ƒæ–‡ä»¶:")
    print(f"  ğŸ¯ all_samples_predictions.txt - Summary file â­â­â­")
    print(f"     (åŒ…å«æ‰€æœ‰{len(all_predictions)}ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ)")
    
    if len(vcf_files) > 1:
        print(f"\nå„VCF subfolder:")
        for vcf_file in vcf_files:
            vcf_basename = vcf_file.name.split('.')[0].split('-')[0].split('_')[0]
            print(f"  ğŸ“‚ {vcf_basename}/")
            print(f"     - {vcf_basename}_predictions.txt")
            print(f"     - detailed_results.csv")
            print(f"     - ancestry_report.txt")
            print(f"     - pca_2d.png, pca_3d.png")
    
    print(f"\nğŸ’¡ å¿«é€ŸæŸ¥çœ‹æ€»ç»“æœ:")
    print(f"   cat {output_path}/all_samples_predictions.txt")
    print(f"\nğŸ’¡ ç»Ÿè®¡å„äººç¾¤æ•°é‡:")
    print(f"   cut -f2 {output_path}/all_samples_predictions.txt | tail -n +2 | sort | uniq -c")
    
    return all_predictions


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='PCA Projection and Ancestry Inference - Project new samples onto trained PCA space (supports single file or batch processing)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example usage:

  # 1. Process single VCF file
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf new_cohort.vcf.gz \\
      --output-dir query_results
  
  # 2. Batch process VCF folder (automatically process all VCFs in folder)ğŸ†•
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf vcf_folder/ \\
      --output-dir batch_results
  
  # 3. Batch processing + multi-processing acceleration (recommended)ğŸš€
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf vcf_folder/ \\
      --output-dir batch_results \\
      --n-jobs 8
  
  # 4. Specify chromosome region
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf new_cohort.vcf.gz \\
      --region 20 \\
      --output-dir query_results
  
  # 5. Adjust KNN neighbors
  python3 pca_projection.py \\
      --snps selected_snps.npz \\
      --model pca_model.pkl \\
      --reference reference_pca_results.csv \\
      --query-vcf new_cohort.vcf.gz \\
      --n-neighbors 30 \\
      --output-dir query_results

Output file structure:

  Single VCF file:
    output_dir/
    â”œâ”€â”€ all_samples_predictions.txt   # Summary file â­â­â­
    â”œâ”€â”€ <vcf>/                        # VCF subfolder
    â”‚   â”œâ”€â”€ <vcf>_predictions.txt     # Prediction for this VCF
    â”‚   â”œâ”€â”€ detailed_results.csv      # Complete results (PCA coords + probabilities)
    â”‚   â”œâ”€â”€ ancestry_report.txt       # Detailed report
    â”‚   â”œâ”€â”€ pca_2d.png               # 2D visualization
    â”‚   â”œâ”€â”€ pca_3d.png               # 3D visualization
    â”‚   â””â”€â”€ pca_3d_interactive.html  # Interactive 3D plot
  
  Batch process VCF folder:
    output_dir/
    â”œâ”€â”€ all_samples_predictions.txt   # Summary fileï¼ˆall samples)â­â­â­
    â”œâ”€â”€ vcf1/                        # Results for 1st VCF
    â”œâ”€â”€ vcf2/                        # Results for 2nd VCF
    â””â”€â”€ vcf3/                        # Results for 3rd VCF

Workflow:
  1. 1. Train PCA model (use 1000genomes_pca_ultimate.py)
     python3 1000genomes_pca_ultimate.py ... --save-snps snps.npz --save-model model.pkl
  
  2. 2. Project new samples (use this script)
     python3 pca_projection.py --snps snps.npz --model model.pkl --reference ref.csv --query-vcf new.vcf.gz
        """
    )
    
    parser.add_argument('--snps', required=True,
                       help='SNP position list file (.npz format)')
    parser.add_argument('--model', required=True,
                       help='PCA model file (.pkl format)')
    parser.add_argument('--reference', required=True,
                       help='Reference population PCA results CSV file')
    parser.add_argument('--query-vcf', required=True,
                       help='Query sample VCF file or folder containing VCF files (supports batch processing)')
    parser.add_argument('-o', '--output-dir', default='projection_output',
                       help='Output folder (default: projection_output)')
    parser.add_argument('-r', '--region',
                       help='Chromosome region (optional, e.g., 20 or 20:1000000-2000000)')
    parser.add_argument('--n-neighbors', type=int, default=20,
                       help='Number of KNN neighbors (default: 20)')
    parser.add_argument('--n-jobs', type=int, default=1,
                       help='Number of parallel processes (default: 1). For multiple VCFs, recommend setting to CPU count, e.g., --n-jobs 8')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    for file, name in [(args.snps, 'SNP file'), (args.model, 'Model file'), 
                       (args.reference, 'ReferenceCSV'), (args.query_vcf, 'QueryVCF')]:
        if not os.path.exists(file):
            print(f"é”™è¯¯: {name}does not exist: {file}")
            exit(1)
    
    try:
        main(
            snps_file=args.snps,
            model_file=args.model,
            reference_csv=args.reference,
            query_vcf_or_dir=args.query_vcf,
            output_dir=args.output_dir,
            region=args.region,
            n_neighbors=args.n_neighbors,
            n_jobs=args.n_jobs
        )
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
