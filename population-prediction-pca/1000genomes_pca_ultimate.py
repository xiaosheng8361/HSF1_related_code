#!/usr/bin/env python3
"""
1000 Genomes PCA Analysis - Ultimate Optimized Version
Memory-optimized version for large-scale whole-genome analysis

Key Features:
1. Chromosome-wise processing - prevents memory overflow
2. Automatic merging - seamlessly combines all chromosome results
3. Resume capability - automatically skips processed chromosomes
4. LD pruning pre-filtering - spatial sampling reduces variants by 10x
"""

import numpy as np
import pandas as pd
import allel
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import os
import gc
import pickle
from pathlib import Path

# å°è¯•å¯¼å…¥plotly
try:
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False

# ç¼“å­˜ç›®å½•
CACHE_DIR = Path(".pca_cache")

# æ ‡å‡†æŸ“è‰²ä½“åˆ—è¡¨
CHROMOSOMES = [str(i) for i in range(1, 23)] + ['X']


def spatial_thinning(allele_counts, positions, bin_size=100000, max_per_bin=20):
    """
    ç©ºé—´ç¨€ç–åŒ–ï¼šåœ¨LDä¿®å‰ªå‰å…ˆåšç©ºé—´ç­›é€‰
    
    ç­–ç•¥ï¼šæ¯100kbèŒƒå›´å†…ï¼Œå¦‚æœè¶…è¿‡max_per_binä¸ªä½ç‚¹ï¼ŒéšæœºæŠ½æ ·
    
    å‚æ•°:
        allele_counts: ç­‰ä½åŸºå› çŸ©é˜µ (samples x variants)
        positions: ä½ç‚¹ä½ç½®
        bin_size: åŒºé—´å¤§å°ï¼ˆbpï¼‰
        max_per_bin: æ¯ä¸ªåŒºé—´æœ€å¤šä¿ç•™çš„ä½ç‚¹æ•°
    
    è¿”å›:
        ç­›é€‰åçš„æ•°æ®
    """
    if positions is None or len(positions) == 0:
        return allele_counts, positions
    
    print(f"\nç©ºé—´ç¨€ç–åŒ–é¢„å¤„ç†...")
    print(f"  å‚æ•°: åŒºé—´={bin_size//1000}kb, æ¯åŒºé—´æœ€å¤š{max_per_bin}ä¸ªä½ç‚¹")
    print(f"  è¾“å…¥: {allele_counts.shape[1]} ä¸ªå˜å¼‚ä½ç‚¹")
    
    # è®¡ç®—æ¯ä¸ªä½ç‚¹æ‰€å±çš„åŒºé—´
    bins = positions // bin_size
    unique_bins = np.unique(bins)
    
    selected_indices = []
    
    for bin_id in unique_bins:
        # æ‰¾åˆ°è¯¥åŒºé—´å†…çš„æ‰€æœ‰ä½ç‚¹
        bin_mask = bins == bin_id
        bin_indices = np.where(bin_mask)[0]
        
        # å¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼ŒéšæœºæŠ½æ ·
        if len(bin_indices) > max_per_bin:
            np.random.seed(42)  # ä¿è¯å¯é‡å¤
            selected = np.random.choice(bin_indices, max_per_bin, replace=False)
            selected_indices.extend(selected)
        else:
            selected_indices.extend(bin_indices)
    
    # æ’åºç´¢å¼•
    selected_indices = sorted(selected_indices)
    
    # åº”ç”¨ç­›é€‰
    allele_counts_thinned = allele_counts[:, selected_indices]
    positions_thinned = positions[selected_indices]
    
    print(f"  è¾“å‡º: {len(selected_indices)} ä¸ªä½ç‚¹")
    print(f"  å‡å°‘: {allele_counts.shape[1] - len(selected_indices)} ä¸ªä½ç‚¹ " +
          f"({(1 - len(selected_indices)/allele_counts.shape[1])*100:.1f}%)")
    
    return allele_counts_thinned, positions_thinned


def generate_chr_cache_filename(vcf_file, chromosome, maf_threshold):
    """ç”Ÿæˆå•ä¸ªæŸ“è‰²ä½“çš„ç¼“å­˜æ–‡ä»¶å"""
    cache_name = f"chr_{chromosome}_maf{maf_threshold:.3f}.pkl.gz"
    print(f"chr_{chromosome}_maf{maf_threshold:.3f}.pkl.gz")
    return CACHE_DIR / cache_name


def generate_merged_cache_filename(maf_threshold):
    """ç”Ÿæˆåˆå¹¶åçš„å…¨åŸºå› ç»„ç¼“å­˜æ–‡ä»¶å"""
    cache_name = f"merged_all_chr_maf{maf_threshold:.3f}.pkl.gz"
    return CACHE_DIR / cache_name


def save_chromosome_data(allele_counts, samples, positions, cache_file):
    """ä¿å­˜å•ä¸ªæŸ“è‰²ä½“çš„æ•°æ®"""
    CACHE_DIR.mkdir(exist_ok=True)
    
    cache_data = {
        'allele_counts': allele_counts,
        'samples': samples,
        'positions': positions,
        'shape': allele_counts.shape,
        'timestamp': pd.Timestamp.now().isoformat()
    }
    
    with open(cache_file, 'wb') as f:
        pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    file_size_mb = cache_file.stat().st_size / (1024 * 1024)
    return file_size_mb


def load_chromosome_data(cache_file):
    """åŠ è½½å•ä¸ªæŸ“è‰²ä½“çš„æ•°æ®"""
    with open(cache_file, 'rb') as f:
        cache_data = pickle.load(f)
    
    # ç¡®ä¿positionsæ˜¯int64ç±»å‹ï¼ˆå…¼å®¹æ—§ç¼“å­˜ï¼‰
    positions = cache_data['positions']
    if positions is not None and positions.dtype != np.int64:
        positions = positions.astype(np.int64)
    
    return cache_data['allele_counts'], cache_data['samples'], positions


def read_and_filter_chromosome(vcf_file, chromosome, maf_threshold=0.01, 
                               missing_threshold=0.1, use_cache=True):
    """
    è¯»å–å’Œè¿‡æ»¤å•ä¸ªæŸ“è‰²ä½“çš„æ•°æ®
    æ”¯æŒç¼“å­˜åŠŸèƒ½
    """
    cache_file = generate_chr_cache_filename(vcf_file, chromosome, maf_threshold)
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache and cache_file.exists():
        print(f"\n  ğŸ“‚ ä½¿ç”¨ç¼“å­˜: {cache_file.name}")
        try:
            allele_counts, samples, positions = load_chromosome_data(cache_file)
            print(f"     å½¢çŠ¶: {allele_counts.shape}, ç¼“å­˜åŠ è½½æˆåŠŸ âœ“")
            return allele_counts, samples, positions
        except Exception as e:
            print(f"     ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°è¯»å–")
    
    # è¯»å–VCF
    print(f"\n  ğŸ” è¯»å–æŸ“è‰²ä½“ {chromosome}...")
    try:
        callset = allel.read_vcf(vcf_file, region=chromosome)
    except Exception as e:
        print(f"     è¯»å–å¤±è´¥: {e}")
        return None, None, None
    
    if callset is None or 'calldata/GT' not in callset:
        print(f"     æŸ“è‰²ä½“ {chromosome} æ— æ•°æ®ï¼Œè·³è¿‡")
        return None, None, None
    
    # è½¬æ¢æ•°æ®
    genotypes = allel.GenotypeArray(callset['calldata/GT'])
    gn = genotypes.to_n_alt()
    allele_counts = gn.T  # samples x variants
    # ç¡®ä¿positionsæ˜¯int64ç±»å‹ï¼Œé¿å…åç»­åŠ offsetæ—¶æº¢å‡º
    positions = callset['variants/POS'].astype(np.int64) if 'variants/POS' in callset else None
    samples = callset.get('samples', None)
    
    if samples is None:
        raise ValueError(f"æŸ“è‰²ä½“ {chromosome} æ— samplesä¿¡æ¯")
    
    # ç¡®ä¿samplesæ˜¯å­—ç¬¦ä¸²
    if isinstance(samples[0], bytes):
        samples = np.array([s.decode('utf-8') for s in samples])
    else:
        samples = np.array([str(s) for s in samples])
    
    print(f"     åŸå§‹: {allele_counts.shape[0]} æ ·æœ¬, {allele_counts.shape[1]} å˜å¼‚")
    
    # è¿‡æ»¤
    n_samples = allele_counts.shape[0]
    
    # 1. ç¼ºå¤±ç‡è¿‡æ»¤
    missing_count = np.sum(allele_counts == -1, axis=0)
    missing_rate = missing_count / n_samples
    mask = missing_rate <= missing_threshold
    allele_counts = allele_counts[:, mask]
    if positions is not None:
        positions = positions[mask]
    
    # 2. MAFè¿‡æ»¤
    valid_mask = allele_counts != -1
    allele_sum = np.sum(np.where(valid_mask, allele_counts, 0), axis=0)
    valid_count = np.sum(valid_mask, axis=0) * 2
    
    with np.errstate(divide='ignore', invalid='ignore'):
        af = allele_sum / valid_count
        af[valid_count == 0] = 0
    
    maf = np.minimum(af, 1 - af)
    mask = maf >= maf_threshold
    allele_counts = allele_counts[:, mask]
    if positions is not None:
        positions = positions[mask]
    
    # 3. å•æ€ä½ç‚¹è¿‡æ»¤
    std_vals = np.std(allele_counts, axis=0)
    mask = std_vals > 0
    allele_counts = allele_counts[:, mask]
    if positions is not None:
        positions = positions[mask]
    
    print(f"     è¿‡æ»¤å: {allele_counts.shape[1]} å˜å¼‚")
    
    # ä¿å­˜ç¼“å­˜
    if use_cache and allele_counts.shape[1] > 0:
        file_size = save_chromosome_data(allele_counts, samples, positions, cache_file)
        print(f"     ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {file_size:.1f} MB")
    
    # é‡Šæ”¾å†…å­˜
    del genotypes, gn, callset
    gc.collect()
    
    return allele_counts, samples, positions


def process_all_chromosomes(vcf_file, maf_threshold=0.01, chromosomes=None,
                            use_cache=True):
    """
    å¤„ç†æ‰€æœ‰æŸ“è‰²ä½“å¹¶åˆå¹¶
    """
    if chromosomes is None:
        chromosomes = CHROMOSOMES[:22]  # é»˜è®¤1-22å·æŸ“è‰²ä½“
    
    print("=" * 80)
    print(f"åˆ†æŸ“è‰²ä½“å¤„ç†ï¼šå°†å¤„ç† {len(chromosomes)} ä¸ªæŸ“è‰²ä½“")
    print("=" * 80)
    
    all_allele_counts = []
    all_positions = []
    common_samples = None
    
    for i, chrom in enumerate(chromosomes, 1):
        print(f"\n[{i}/{len(chromosomes)}] æŸ“è‰²ä½“ {chrom}")
        print("-" * 80)
        
        allele_counts, samples, positions = read_and_filter_chromosome(
            vcf_file, chrom, maf_threshold=maf_threshold, use_cache=use_cache
        )
        
        if allele_counts is None or allele_counts.shape[1] == 0:
            print(f"     âš ï¸  è·³è¿‡æŸ“è‰²ä½“ {chrom}")
            continue
        
        # æ£€æŸ¥æ ·æœ¬ä¸€è‡´æ€§
        if common_samples is None:
            common_samples = samples
        elif not np.array_equal(common_samples, samples):
            print(f"     âš ï¸  è­¦å‘Šï¼šæŸ“è‰²ä½“ {chrom} çš„æ ·æœ¬ä¸ä¹‹å‰ä¸ä¸€è‡´ï¼Œè·³è¿‡")
            continue
        
        # æ·»åŠ æŸ“è‰²ä½“ä¿¡æ¯åˆ°positions
        if positions is not None:
            # ä¸ºæ¯ä¸ªæŸ“è‰²ä½“çš„positionæ·»åŠ åç§»ï¼Œé¿å…é‡å 
            # positionså·²ç»åœ¨è¯»å–æ—¶è½¬æ¢ä¸ºint64ï¼Œè¿™é‡Œç›´æ¥è®¡ç®—offset
            chrom_offset = int(chrom) * 1_000_000_000 if chrom.isdigit() else 0
            positions_offset = positions + chrom_offset
            all_positions.append(positions_offset)
        
        all_allele_counts.append(allele_counts)
        
        # æ˜¾ç¤ºè¿›åº¦
        total_variants = sum(ac.shape[1] for ac in all_allele_counts)
        print(f"     âœ“ ç´¯è®¡å˜å¼‚æ•°: {total_variants}")
    
    if len(all_allele_counts) == 0:
        raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æŸ“è‰²ä½“ï¼")
    
    # åˆå¹¶æ‰€æœ‰æŸ“è‰²ä½“
    print("\n" + "=" * 80)
    print("åˆå¹¶æ‰€æœ‰æŸ“è‰²ä½“æ•°æ®...")
    print("=" * 80)
    
    merged_allele_counts = np.concatenate(all_allele_counts, axis=1)
    # ç¡®ä¿åˆå¹¶åçš„positionsæ˜¯int64ç±»å‹
    merged_positions = np.concatenate(all_positions).astype(np.int64) if all_positions else None
    
    print(f"  åˆå¹¶å: {merged_allele_counts.shape[0]} æ ·æœ¬, " +
          f"{merged_allele_counts.shape[1]} å˜å¼‚ä½ç‚¹")
    
    # ä¿å­˜åˆå¹¶åçš„ç¼“å­˜
    if use_cache:
        merged_cache_file = generate_merged_cache_filename(maf_threshold)
        print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ•°æ®...")
        file_size = save_chromosome_data(
            merged_allele_counts, common_samples, merged_positions, merged_cache_file
        )
        print(f"  ç¼“å­˜æ–‡ä»¶: {merged_cache_file.name}")
        print(f"  æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
    
    # é‡Šæ”¾å†…å­˜
    del all_allele_counts
    gc.collect()
    
    return merged_allele_counts, common_samples, merged_positions


def ld_prune_parallel_chunk(args):
    """å¹¶è¡ŒLDä¿®å‰ªçš„å•ä¸ªchunkå¤„ç†å‡½æ•°"""
    gn_chunk, positions_chunk, window_bp, threshold = args
    try:
        chunk_mask = allel.locate_unlinked(
            gn_chunk,
            size=window_bp,
            step=window_bp,
            threshold=threshold,
            n_iter=1
        )
    except TypeError:
        chunk_mask = allel.locate_unlinked(
            gn_chunk,
            size=window_bp,
            step=window_bp,
            threshold=threshold
        )
    return chunk_mask


def ld_prune_with_thinning(allele_counts, positions, window_size=500, 
                           threshold=0.2, thin_bin_size=100000, 
                           max_per_bin=20, n_jobs=4):
    """
    å¸¦ç©ºé—´ç¨€ç–åŒ–çš„LDä¿®å‰ªï¼ˆæ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œï¼‰
    
    ç­–ç•¥ï¼š
    1. å…ˆç©ºé—´ç¨€ç–åŒ–ï¼ˆæ¯100kbæœ€å¤š20ä¸ªSNPï¼‰
    2. å†è¿›è¡ŒLDä¿®å‰ªï¼ˆæ”¯æŒå¤šè¿›ç¨‹åŠ é€Ÿï¼‰
    
    å‚æ•°:
        n_jobs: å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œé»˜è®¤4ã€‚è®¾ä¸º1åˆ™å•çº¿ç¨‹ï¼Œ-1åˆ™ä½¿ç”¨æ‰€æœ‰CPU
    """
    if positions is None:
        print("\nè­¦å‘Š: æ— ä½ç½®ä¿¡æ¯ï¼Œè·³è¿‡LDä¿®å‰ª")
        return allele_counts
    
    print(f"\nLDä¿®å‰ªï¼ˆå¸¦ç©ºé—´é¢„ç­›é€‰ï¼‰...")
    print(f"  è¾“å…¥: {allele_counts.shape[1]} ä¸ªå˜å¼‚ä½ç‚¹")
    
    # æ­¥éª¤1ï¼šç©ºé—´ç¨€ç–åŒ–
    allele_counts_thin, positions_thin = spatial_thinning(
        allele_counts, positions, 
        bin_size=thin_bin_size, 
        max_per_bin=max_per_bin
    )
    
    # æ­¥éª¤2ï¼šLDä¿®å‰ª
    print(f"\n  æ‰§è¡ŒLDä¿®å‰ª...")
    print(f"  å‚æ•°: çª—å£={window_size}kb, rÂ²é˜ˆå€¼={threshold}")
    
    try:
        # è½¬ç½®
        gn_for_ld = allele_counts_thin.T
        
        window_bp = window_size * 1000
        
        # è‡ªé€‚åº”æ­¥é•¿
        if allele_counts_thin.shape[1] > 50000:
            step_bp = window_bp  # ä¸é‡å 
            print(f"  ä½¿ç”¨å¿«é€Ÿæ­¥é•¿: {window_size}kb (ä¸é‡å )")
        else:
            step_bp = int(window_bp / 2)  # 50%é‡å 
            print(f"  ä½¿ç”¨æ ‡å‡†æ­¥é•¿: {window_size//2}kb (50%é‡å )")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¹¶è¡Œ
        import os
        if n_jobs == -1:
            n_jobs = os.cpu_count()
        
        use_parallel = n_jobs > 1 and allele_counts_thin.shape[1] > 100000
        
        if use_parallel:
            print(f"  ğŸš€ å¯ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ (è¿›ç¨‹æ•°: {n_jobs})")
            print(f"  æ­£åœ¨è®¡ç®—LDç›¸å…³æ€§...")
            
            from multiprocessing import Pool
            
            # åˆ†å—å¤„ç†
            n_variants = gn_for_ld.shape[0]
            chunk_size = max(10000, n_variants // (n_jobs * 2))
            
            chunks = []
            for i in range(0, n_variants, chunk_size):
                end_idx = min(i + chunk_size, n_variants)
                gn_chunk = gn_for_ld[i:end_idx]
                pos_chunk = positions_thin[i:end_idx]
                chunks.append((gn_chunk, pos_chunk, window_bp, threshold))
            
            print(f"  åˆ†ä¸º {len(chunks)} ä¸ªå—å¹¶è¡Œå¤„ç†...")
            
            # å¹¶è¡Œå¤„ç†
            with Pool(processes=n_jobs) as pool:
                chunk_masks = pool.map(ld_prune_parallel_chunk, chunks)
            
            # åˆå¹¶ç»“æœ
            ld_mask = np.concatenate(chunk_masks)
        else:
            if n_jobs > 1:
                print(f"  å˜å¼‚æ•°è¾ƒå°‘ï¼Œä½¿ç”¨å•çº¿ç¨‹å¤„ç†")
            print(f"  æ­£åœ¨è®¡ç®—LDç›¸å…³æ€§...")
            
            # å•çº¿ç¨‹æ‰§è¡ŒLDä¿®å‰ª
            try:
                ld_mask = allel.locate_unlinked(
                    gn_for_ld,
                    size=window_bp,
                    step=step_bp,
                    threshold=threshold,
                    n_iter=1
                )
            except TypeError:
                ld_mask = allel.locate_unlinked(
                    gn_for_ld,
                    size=window_bp,
                    step=step_bp,
                    threshold=threshold
                )
        
        n_after = np.sum(ld_mask)
        removed = allele_counts_thin.shape[1] - n_after
        
        print(f"  âœ“ LDä¿®å‰ªå®Œæˆ")
        print(f"  è¾“å‡º: {n_after} ä¸ªç‹¬ç«‹å˜å¼‚ä½ç‚¹")
        print(f"  ç§»é™¤: {removed} ä¸ªç›¸å…³å˜å¼‚ ({removed/allele_counts_thin.shape[1]*100:.1f}%)")
        
        allele_counts_pruned = allele_counts_thin[:, ld_mask]
        positions_pruned = positions_thin[ld_mask]
        
        # æ€»ç»“
        total_removed = allele_counts.shape[1] - allele_counts_pruned.shape[1]
        print(f"\n  æ€»è®¡: {allele_counts.shape[1]} â†’ {allele_counts_pruned.shape[1]} " +
              f"(å‡å°‘ {total_removed/allele_counts.shape[1]*100:.1f}%)")
        
        return allele_counts_pruned, positions_pruned
    
    except Exception as e:
        print(f"  âœ— LDä¿®å‰ªå¤±è´¥: {e}")
        print(f"  â†’ è¿”å›ç©ºé—´ç¨€ç–åŒ–åçš„æ•°æ®")
        return allele_counts_thin, positions_thin


def run_pca_analysis(allele_counts, n_components=3):
    """æ‰§è¡ŒPCAåˆ†æ"""
    print(f"\nä½¿ç”¨ {n_components} ä¸ªä¸»æˆåˆ†è¿è¡ŒPCA...")
    print(f"  è¾“å…¥çŸ©é˜µ: {allele_counts.shape[0]} æ ·æœ¬ Ã— {allele_counts.shape[1]} å˜å¼‚")
    
    # å¤„ç†ç¼ºå¤±å€¼
    allele_counts_clean = allele_counts.copy().astype(float)
    for i in range(allele_counts_clean.shape[1]):
        col = allele_counts_clean[:, i]
        mask = col != -1
        if np.sum(mask) > 0:
            mean_val = np.mean(col[mask])
            col[~mask] = mean_val
    
    # æ ‡å‡†åŒ–
    mean_vals = np.mean(allele_counts_clean, axis=0)
    std_vals = np.std(allele_counts_clean, axis=0)
    std_vals[std_vals == 0] = 1
    
    allele_counts_scaled = (allele_counts_clean - mean_vals) / std_vals
    
    # PCA
    pca = PCA(n_components=n_components)
    pca_result = pca.fit_transform(allele_counts_scaled)
    
    print(f"\nä¸»æˆåˆ†æ–¹å·®è§£é‡Šç‡:")
    for i, var in enumerate(pca.explained_variance_ratio_):
        print(f"  PC{i+1}: {var:.4f} ({var*100:.2f}%)")
    print(f"  ç´¯è®¡: {sum(pca.explained_variance_ratio_):.4f} " +
          f"({sum(pca.explained_variance_ratio_)*100:.2f}%)")
    
    # è¿”å›PCAæ¨¡å‹å’Œæ ‡å‡†åŒ–å‚æ•°
    normalization_params = {
        'mean': mean_vals,
        'std': std_vals
    }
    
    return pca_result, pca, normalization_params


def plot_pca_results(pca_result, samples, pop_data=None, output_prefix='pca'):
    """ç»˜åˆ¶PCAç»“æœ"""
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    n_components = pca_result.shape[1]
    pca_df = pd.DataFrame(
        pca_result,
        columns=[f'PC{i+1}' for i in range(n_components)]
    )
    pca_df['Sample'] = samples
    
    # åˆå¹¶äººç¾¤ä¿¡æ¯
    if pop_data is not None:
        pca_df['Sample'] = pca_df['Sample'].astype(str)
        pop_data_copy = pop_data.copy()
        pop_data_copy['sample'] = pop_data_copy['sample'].astype(str)
        
        pca_df = pca_df.merge(
            pop_data_copy,
            left_on='Sample',
            right_on='sample',
            how='left'
        )
        
        matched = pca_df['super_pop'].notna().sum()
        if matched > 0:
            print(f"\näººç¾¤åˆ†å¸ƒ:")
            print(pca_df['super_pop'].value_counts().to_string())
    
    # é¢œè‰²å®šä¹‰
    super_pop_colors = {
        'AFR': '#E74C3C',  # çº¢è‰²
        'AMR': '#9B59B6',  # ç´«è‰²
        'EAS': '#3498DB',  # è“è‰²
        'EUR': '#F39C12',  # æ©™è‰²
        'SAS': '#2ECC71'   # ç»¿è‰²
    }
    
    # 2Då›¾
    fig, ax = plt.subplots(figsize=(14, 10))
    
    if pop_data is not None and 'super_pop' in pca_df.columns:
        for super_pop in sorted(pca_df['super_pop'].dropna().unique()):
            mask = pca_df['super_pop'] == super_pop
            ax.scatter(
                pca_df.loc[mask, 'PC1'],
                pca_df.loc[mask, 'PC2'],
                label=super_pop,
                alpha=0.7,
                s=50,
                color=super_pop_colors.get(super_pop, '#999999'),
                edgecolors='white',
                linewidths=0.5
            )
        ax.legend(title='Super Population', fontsize=12)
    else:
        ax.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.6, s=50)
    
    ax.set_xlabel('PC1', fontsize=14, fontweight='bold')
    ax.set_ylabel('PC2', fontsize=14, fontweight='bold')
    ax.set_title('1000 Genomes PCA - PC1 vs PC2', fontsize=16, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{output_prefix}_pc1_pc2.png', dpi=300, bbox_inches='tight')
    print(f"  ä¿å­˜: {output_prefix}_pc1_pc2.png")
    plt.close()
    
    # 3Dé™æ€å›¾
    if n_components >= 3:
        fig = plt.figure(figsize=(16, 12))
        ax = fig.add_subplot(111, projection='3d')
        
        if pop_data is not None and 'super_pop' in pca_df.columns:
            for super_pop in sorted(pca_df['super_pop'].dropna().unique()):
                mask = pca_df['super_pop'] == super_pop
                ax.scatter(
                    pca_df.loc[mask, 'PC1'],
                    pca_df.loc[mask, 'PC2'],
                    pca_df.loc[mask, 'PC3'],
                    c=super_pop_colors.get(super_pop, '#999999'),
                    label=super_pop,
                    alpha=0.7,
                    s=50
                )
            ax.legend(title='Super Population')
        else:
            ax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'], alpha=0.6, s=50)
        
        ax.set_xlabel('PC1', fontweight='bold')
        ax.set_ylabel('PC2', fontweight='bold')
        ax.set_zlabel('PC3', fontweight='bold')
        ax.set_title('1000 Genomes PCA - 3D', fontsize=15, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{output_prefix}_3d.png', dpi=300, bbox_inches='tight')
        print(f"  ä¿å­˜: {output_prefix}_3d.png")
        plt.close()
        
        # äº¤äº’å¼3Då›¾
        if PLOTLY_AVAILABLE and pop_data is not None and 'super_pop' in pca_df.columns:
            print("  ç”Ÿæˆäº¤äº’å¼3Då›¾...")
            
            fig_plotly = go.Figure()
            for super_pop in sorted(pca_df['super_pop'].dropna().unique()):
                mask = pca_df['super_pop'] == super_pop
                df_subset = pca_df[mask]
                
                fig_plotly.add_trace(go.Scatter3d(
                    x=df_subset['PC1'],
                    y=df_subset['PC2'],
                    z=df_subset['PC3'],
                    mode='markers',
                    name=super_pop,
                    marker=dict(
                        size=5,
                        color=super_pop_colors.get(super_pop, '#999999'),
                        opacity=0.7
                    ),
                    text=df_subset['Sample'].astype(str),
                    hovertemplate='<b>%{text}</b><br>PC1: %{x:.3f}<br>PC2: %{y:.3f}<br>PC3: %{z:.3f}<extra></extra>'
                ))
            
            fig_plotly.update_layout(
                title='1000 Genomes PCA - äº¤äº’å¼3Då›¾',
                scene=dict(
                    xaxis=dict(title='PC1'),
                    yaxis=dict(title='PC2'),
                    zaxis=dict(title='PC3')
                ),
                width=1200,
                height=900
            )
            
            html_file = f'{output_prefix}_3d_interactive.html'
            fig_plotly.write_html(html_file)
            print(f"  ä¿å­˜: {html_file}")
    
    # ä¿å­˜CSV
    pca_df.to_csv(f'{output_prefix}_results.csv', index=False)
    print(f"  ä¿å­˜: {output_prefix}_results.csv")
    
    return pca_df


def main(vcf_file, pop_file=None, output_prefix='pca_ultimate', n_components=3,
         region=None, maf_threshold=0.01, enable_ld_prune=True,
         ld_window_size=500, ld_threshold=0.2, use_cache=True,
         force_refresh=False, thin_bin_size=100, max_per_bin=20, n_jobs=4,
         save_snps=None, save_model=None):
    """
    ä¸»å‡½æ•° - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬
    
    å‚æ•°:
        n_jobs: LDä¿®å‰ªå¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆ1=å•çº¿ç¨‹ï¼Œ-1=æ‰€æœ‰CPUï¼Œé»˜è®¤4ï¼‰
        save_snps: ä¿å­˜é€‰ä¸­çš„SNPä½ç½®åˆ—è¡¨çš„æ–‡ä»¶åï¼ˆç”¨äºæŠ•å½±ï¼‰
        save_model: ä¿å­˜PCAæ¨¡å‹å’Œæ ‡å‡†åŒ–å‚æ•°çš„æ–‡ä»¶åï¼ˆç”¨äºæŠ•å½±ï¼‰
    """
    print("=" * 80)
    print("1000 Genomes PCA åˆ†æ - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬")
    print("æ”¯æŒå¤§è§„æ¨¡å…¨åŸºå› ç»„åˆ†æ")
    print("=" * 80)
    
    print("\nä¼˜åŒ–ç‰¹æ€§:")
    print("  âœ“ åˆ†æŸ“è‰²ä½“å¤„ç† - é¿å…å†…å­˜çˆ†ç‚¸")
    print("  âœ“ æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ - æ–­ç‚¹ç»­ä¼ ")
    print("  âœ“ ç©ºé—´é¢„ç­›é€‰ - LDä¿®å‰ªå‰å‡å°‘10å€ä½ç‚¹")
    print("  âœ“ è‡ªåŠ¨åˆå¹¶ - æ— ç¼æ•´åˆæ‰€æœ‰æŸ“è‰²ä½“")
    
    print("\nåˆ†æå‚æ•°:")
    print(f"  ä¸»æˆåˆ†æ•°: {n_components}")
    print(f"  MAFé˜ˆå€¼: {maf_threshold}")
    print(f"  LDä¿®å‰ª: {'å¯ç”¨' if enable_ld_prune else 'ç¦ç”¨'}")
    if enable_ld_prune:
        print(f"    çª—å£: {ld_window_size}kb, rÂ²<{ld_threshold}")
        print(f"    é¢„ç­›é€‰: æ¯{thin_bin_size}kbæœ€å¤š{max_per_bin}ä¸ªSNP")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å…¨åŸºå› ç»„åˆ†æ
    is_whole_genome = region is None
    
    if is_whole_genome:
        print(f"\nğŸŒ å…¨åŸºå› ç»„æ¨¡å¼")
        
        # æ£€æŸ¥åˆå¹¶ç¼“å­˜
        merged_cache = generate_merged_cache_filename(maf_threshold)
        if use_cache and not force_refresh and merged_cache.exists():
            print(f"\nâœ“ å‘ç°å…¨åŸºå› ç»„åˆå¹¶ç¼“å­˜: {merged_cache.name}")
            try:
                allele_counts, samples, positions = load_chromosome_data(merged_cache)
                print(f"  æ•°æ®å½¢çŠ¶: {allele_counts.shape}")
                print(f"  ğŸ“‚ ä½¿ç”¨åˆå¹¶ç¼“å­˜ï¼Œè·³è¿‡æ‰€æœ‰æŸ“è‰²ä½“å¤„ç† âš¡")
            except Exception as e:
                print(f"  ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°å¤„ç†")
                allele_counts, samples, positions = process_all_chromosomes(
                    vcf_file, maf_threshold, use_cache=use_cache
                )
        else:
            # åˆ†æŸ“è‰²ä½“å¤„ç†
            allele_counts, samples, positions = process_all_chromosomes(
                vcf_file, maf_threshold, use_cache=use_cache
            )
    else:
        print(f"\nğŸ“ åŒºåŸŸæ¨¡å¼: {region}")
        
        # å•ä¸ªåŒºåŸŸ/æŸ“è‰²ä½“å¤„ç†
        from pathlib import Path
        cache_file = CACHE_DIR / f"region_{region.replace(':', '_')}_maf{maf_threshold:.3f}.pkl.gz"
        
        if use_cache and not force_refresh and cache_file.exists():
            print(f"\nâœ“ ä½¿ç”¨ç¼“å­˜")
            allele_counts, samples, positions = load_chromosome_data(cache_file)
        else:
            # è¯»å–å’Œè¿‡æ»¤
            allele_counts, samples, positions = read_and_filter_chromosome(
                vcf_file, region, maf_threshold, use_cache=use_cache
            )
    
    # è¯»å–äººç¾¤ä¿¡æ¯
    pop_data = None
    if pop_file and os.path.exists(pop_file):
        print(f"\nè¯»å–äººç¾¤ä¿¡æ¯: {pop_file}")
        pop_data = pd.read_csv(pop_file, sep='\t')
        pop_data['sample'] = pop_data['sample'].astype(str)
        print(f"  åŠ è½½: {len(pop_data)} ä¸ªæ ·æœ¬")
    
    # LDä¿®å‰ª
    if enable_ld_prune:
        allele_counts, positions = ld_prune_with_thinning(
            allele_counts, positions,
            window_size=ld_window_size,
            threshold=ld_threshold,
            thin_bin_size=thin_bin_size * 1000,
            max_per_bin=max_per_bin,
            n_jobs=n_jobs
        )
    
    # PCA
    pca_result, pca_model, norm_params = run_pca_analysis(allele_counts, n_components)
    
    # ä¿å­˜SNPä½ç½®ï¼ˆç”¨äºæŠ•å½±ï¼‰
    if save_snps and positions is not None:
        print(f"\nğŸ’¾ ä¿å­˜SNPä½ç½®åˆ—è¡¨...")
        np.savez_compressed(save_snps, positions=positions)
        print(f"  ä¿å­˜: {save_snps}")
        print(f"  ä½ç‚¹æ•°: {len(positions)}")
    
    # ä¿å­˜PCAæ¨¡å‹å’Œæ ‡å‡†åŒ–å‚æ•°ï¼ˆç”¨äºæŠ•å½±ï¼‰
    if save_model:
        print(f"\nğŸ’¾ ä¿å­˜PCAæ¨¡å‹å’Œæ ‡å‡†åŒ–å‚æ•°...")
        model_data = {
            'pca_model': pca_model,
            'mean': norm_params['mean'],
            'std': norm_params['std'],
            'n_components': n_components,
            'n_features': allele_counts.shape[1]
        }
        with open(save_model, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"  ä¿å­˜: {save_model}")
        print(f"  ä¸»æˆåˆ†æ•°: {n_components}")
        print(f"  ç‰¹å¾æ•°: {allele_counts.shape[1]}")
    
    # å¯è§†åŒ–
    pca_df = plot_pca_results(pca_result, samples, pop_data, output_prefix)
    
    print("\n" + "=" * 80)
    print("åˆ†æå®Œæˆï¼")
    print("=" * 80)
    
    if save_snps or save_model:
        print("\nğŸ“¦ å·²ä¿å­˜æŠ•å½±æ‰€éœ€æ–‡ä»¶:")
        if save_snps:
            print(f"  âœ“ SNPä½ç½®: {save_snps}")
        if save_model:
            print(f"  âœ“ PCAæ¨¡å‹: {save_model}")
        print(f"\nğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŠ•å½±æ–°æ ·æœ¬:")
        print(f"   python3 pca_projection.py \\")
        print(f"       --snps {save_snps or 'snps.npz'} \\")
        print(f"       --model {save_model or 'model.pkl'} \\")
        print(f"       --reference {output_prefix}_results.csv \\")
        print(f"       --query-vcf YOUR_VCF.gz \\")
        print(f"       --output query_projection")
    
    return pca_df, pca_model


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='1000 Genomes PCA ç»ˆæä¼˜åŒ–ç‰ˆæœ¬ - æ”¯æŒå¤§è§„æ¨¡å…¨åŸºå› ç»„åˆ†æ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å…¨åŸºå› ç»„åˆ†æï¼ˆè‡ªåŠ¨åˆ†æŸ“è‰²ä½“å¤„ç†ï¼‰
  python3 1000genomes_pca_ultimate.py \\
      all.1kg.phase3_shapeit2_mvncall_integrated_v1b.20130502.vcf.gz \\
      integrated_call_samples_v3.20130502.ALL.panel \\
      --maf 0.05 \\
      --output full_genome
  
  # å•æŸ“è‰²ä½“
  python3 1000genomes_pca_ultimate.py \\
      all.1kg.phase3_shapeit2_mvncall_integrated_v1b.20130502.vcf.gz \\
      integrated_call_samples_v3.20130502.ALL.panel \\
      --region 20 \\
      --output chr20
  
  # è‡ªå®šä¹‰ç©ºé—´ç­›é€‰å‚æ•°
  python3 1000genomes_pca_ultimate.py \\
      all.1kg.phase3_shapeit2_mvncall_integrated_v1b.20130502.vcf.gz \\
      integrated_call_samples_v3.20130502.ALL.panel \\
      --region 20 \\
      --thin-bin 50 --max-per-bin 30
  
  # ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€ŸLDä¿®å‰ªï¼ˆæ¨èï¼‰
  python3 1000genomes_pca_ultimate.py \\
      all.1kg.phase3_shapeit2_mvncall_integrated_v1b.20130502.vcf.gz \\
      integrated_call_samples_v3.20130502.ALL.panel \\
      --maf 0.15 --max-per-bin 10 \\
      --n-jobs 8 \\
      --output fast_result

ä¼˜åŒ–ç‰¹æ€§:
  âœ“ åˆ†æŸ“è‰²ä½“å¤„ç† - é¿å…å…¨åŸºå› ç»„å†…å­˜çˆ†ç‚¸
  âœ“ æ™ºèƒ½ç¼“å­˜ - æ¯ä¸ªæŸ“è‰²ä½“ç‹¬ç«‹ç¼“å­˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
  âœ“ ç©ºé—´é¢„ç­›é€‰ - LDä¿®å‰ªå‰å‡å°‘90%ä½ç‚¹ï¼Œé¿å…å†…å­˜æº¢å‡º
  âœ“ è‡ªåŠ¨åˆå¹¶ - æ— ç¼æ•´åˆæ‰€æœ‰æŸ“è‰²ä½“æ•°æ®
  âœ“ å†…å­˜ä¼˜åŒ– - é€‚åˆ8GBå†…å­˜çš„æœºå™¨è¿è¡Œå…¨åŸºå› ç»„
        """
    )
    
    parser.add_argument('vcf_file', help='VCFæ–‡ä»¶è·¯å¾„')
    parser.add_argument('pop_file', nargs='?', help='äººç¾¤ä¿¡æ¯æ–‡ä»¶')
    parser.add_argument('-o', '--output', default='pca_ultimate',
                       help='è¾“å‡ºå‰ç¼€')
    parser.add_argument('-n', '--n-components', type=int, default=3,
                       help='ä¸»æˆåˆ†æ•°é‡')
    parser.add_argument('-r', '--region',
                       help='æŸ“è‰²ä½“åŒºåŸŸï¼ˆç•™ç©º=å…¨åŸºå› ç»„ï¼‰')
    parser.add_argument('--maf', type=float, default=0.01,
                       help='MAFé˜ˆå€¼')
    parser.add_argument('--no-ld-prune', action='store_true',
                       help='ç¦ç”¨LDä¿®å‰ª')
    parser.add_argument('--ld-window', type=int, default=500,
                       help='LDçª—å£å¤§å°ï¼ˆkbï¼‰')
    parser.add_argument('--ld-threshold', type=float, default=0.2,
                       help='LD rÂ²é˜ˆå€¼')
    parser.add_argument('--thin-bin', type=int, default=100,
                       help='ç©ºé—´ç­›é€‰åŒºé—´å¤§å°ï¼ˆkbï¼‰ï¼Œé»˜è®¤100')
    parser.add_argument('--max-per-bin', type=int, default=20,
                       help='æ¯ä¸ªåŒºé—´æœ€å¤šSNPæ•°ï¼Œé»˜è®¤20')
    parser.add_argument('--n-jobs', type=int, default=4,
                       help='LDä¿®å‰ªå¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆ1=å•çº¿ç¨‹ï¼Œ-1=æ‰€æœ‰CPUï¼Œé»˜è®¤4ï¼‰')
    parser.add_argument('--no-cache', action='store_true',
                       help='ç¦ç”¨ç¼“å­˜')
    parser.add_argument('--force-refresh', action='store_true',
                       help='å¼ºåˆ¶åˆ·æ–°ï¼Œå¿½ç•¥ç¼“å­˜')
    parser.add_argument('--save-snps',
                       help='ä¿å­˜é€‰ä¸­çš„SNPä½ç½®åˆ—è¡¨ï¼ˆç”¨äºæŠ•å½±æ–°æ ·æœ¬ï¼‰')
    parser.add_argument('--save-model',
                       help='ä¿å­˜PCAæ¨¡å‹å’Œæ ‡å‡†åŒ–å‚æ•°ï¼ˆç”¨äºæŠ•å½±æ–°æ ·æœ¬ï¼‰')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.vcf_file):
        print(f"é”™è¯¯: VCFæ–‡ä»¶ä¸å­˜åœ¨: {args.vcf_file}")
        exit(1)
    
    try:
        main(
            vcf_file=args.vcf_file,
            pop_file=args.pop_file,
            output_prefix=args.output,
            n_components=args.n_components,
            region=args.region,
            maf_threshold=args.maf,
            enable_ld_prune=not args.no_ld_prune,
            ld_window_size=args.ld_window,
            ld_threshold=args.ld_threshold,
            use_cache=not args.no_cache,
            force_refresh=args.force_refresh,
            thin_bin_size=args.thin_bin,
            max_per_bin=args.max_per_bin,
            n_jobs=args.n_jobs,
            save_snps=args.save_snps,
            save_model=args.save_model
        )
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
